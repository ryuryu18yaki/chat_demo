# src/rag_preprocess.py

from __future__ import annotations
from io import BytesIO
from typing import List, Dict, Any
import hashlib

import pdfplumber       # pip install pdfplumber
from pdfminer.high_level import extract_text  # type: ignore
from pdfminer.layout import LAParams         # type: ignore
from pypdf import PdfReader
from PIL import Image
from src.logging_utils import init_logger
logger = init_logger()

__all__ = [
    "extract_text_from_pdf",
    "extract_text_from_txt",
    "chunk_text",
    "extract_tables_from_pdf",
    "extract_images_from_pdf",
    "preprocess_files",
]

# ---------------------------------------------------------------------------
# 1) ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
# ---------------------------------------------------------------------------
def extract_text_from_pdf(data: bytes) -> str:
    """PDF ãƒã‚¤ãƒŠãƒªã‹ã‚‰å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ã€‚"""
    with BytesIO(data) as buf:
        laparams = LAParams()
        text = extract_text(buf, laparams=laparams)
    return text

def extract_text_from_txt(data: bytes, encoding: str | None = None) -> str:
    """TXT ãƒã‚¤ãƒŠãƒªã‚’æ–‡å­—åˆ—ã¸ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã€‚"""
    if encoding is None:
        for enc in ("utf-8", "shift_jis", "latin-1"):
            try:
                return data.decode(enc)
            except UnicodeDecodeError:
                continue
        raise UnicodeDecodeError("Failed to decode text file with common encodings")
    return data.decode(encoding)

# ğŸ”¥ ä¿®æ­£ç‰ˆ: ãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
def extract_text_from_pdf_by_pages(data: bytes) -> List[Dict[str, Any]]:
    """PDFã‹ã‚‰ãƒšãƒ¼ã‚¸åˆ¥ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    pages_text = []
    with pdfplumber.open(BytesIO(data)) as pdf:
        for page_num, page in enumerate(pdf.pages, start=1):
            page_text = page.extract_text() or ""
            if page_text.strip():  # ç©ºãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—
                pages_text.append({
                    "text": page_text,
                    "page": page_num
                })
    return pages_text

def should_include_page_numbers(filename: str) -> bool:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«åã«åŸºã¥ã„ã¦ãƒšãƒ¼ã‚¸ç•ªå·ã‚’å«ã‚ã‚‹ã‹ã©ã†ã‹ã‚’æ±ºå®š
    å¿…è¦ã«å¿œã˜ã¦ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ã¦ãã ã•ã„
    """
    # ä¾‹ï¼šç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒšãƒ¼ã‚¸ç•ªå·ãªã—
    no_page_keywords = ["æš—é»™çŸ¥ãƒ¡ãƒ¢"]
    filename_lower = filename.lower()
    
    for keyword in no_page_keywords:
        if keyword in filename_lower:
            return False
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒšãƒ¼ã‚¸ç•ªå·ã‚ã‚Š
    return True

# ---------------------------------------------------------------------------
# 2) ãƒãƒ£ãƒ³ã‚¯åŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆä¿®æ­£ç‰ˆï¼‰
# ---------------------------------------------------------------------------
def chunk_text(text: str, *, chunk_size: int = 800, overlap: int = 80) -> List[str]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’é‡è¤‡ä»˜ãã§åˆ†å‰²ã™ã‚‹ã€‚ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’10%ã«èª¿æ•´ï¼‰"""
    if chunk_size <= overlap:
        raise ValueError("chunk_size must be larger than overlap")
    
    # ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚„çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆ
    if not text.strip() or len(text) <= chunk_size:
        return [text] if text.strip() else []
    
    chunks: List[str] = []
    start = 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunk = text[start:end].strip()
        
        if chunk:  # ç©ºã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—
            chunks.append(chunk)
        
        if end == len(text):
            break
        start = end - overlap
    
    return chunks

# ğŸ”¥ æ–°æ©Ÿèƒ½: ãƒ¦ãƒ‹ãƒ¼ã‚¯IDç”Ÿæˆ
def generate_chunk_id(source: str, page: int, chunk_index: int, content: str) -> str:
    """ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒãƒ£ãƒ³ã‚¯IDã‚’ç”Ÿæˆ"""
    content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
    return f"{source}_p{page}_{chunk_index}_{content_hash}"

# ---------------------------------------------------------------------------
# 3) è¡¨ã®æŠ½å‡º
# ---------------------------------------------------------------------------
def extract_tables_from_pdf(data: bytes) -> List[Dict[str, Any]]:
    """pdfplumber ã§è¡¨ã‚’æŠ½å‡ºã—ã€CSV ãƒ©ã‚¤ã‚¯ãªæ–‡å­—åˆ—ã§è¿”ã™ã€‚"""
    tables: List[Dict[str, Any]] = []
    with pdfplumber.open(BytesIO(data)) as pdf:
        for page_num, page in enumerate(pdf.pages, start=1):
            for tbl_idx, table in enumerate(page.extract_tables(), start=1):
                # None ã‚’ "" ã«ç½®ãæ›ãˆã¦ã‹ã‚‰çµåˆ
                lines: list[str] = []
                for row in table:
                    cells = [(cell if cell is not None else "") for cell in row]
                    lines.append(",".join(cells))
                csv_text = "\n".join(lines)
                
                if csv_text.strip():  # ç©ºã®è¡¨ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    tables.append({
                        "text": csv_text,
                        "page": page_num,
                        "table_id": tbl_idx,
                    })
    return tables

# ---------------------------------------------------------------------------
# 4) ç”»åƒã®æŠ½å‡º
# ---------------------------------------------------------------------------
def extract_images_from_pdf(pdf_bytes: bytes) -> List[Dict[str, Any]]:
    reader = PdfReader(BytesIO(pdf_bytes))
    images, counter = [], 0

    for pnum, page in enumerate(reader.pages, start=1):
        for img_obj in page.images:
            counter += 1
            data = img_obj.data

            # --- Pillow ã§å½¢å¼åˆ¤å®š ---
            try:
                fmt = Image.open(BytesIO(data)).format.lower()  # 'jpeg', 'png', ...
            except Exception:
                fmt = "png"  # ä½•ã‹ã‚ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            ext = "jpg" if fmt == "jpeg" else fmt

            fname = f"page{pnum}_{counter:03}.{ext}"
            images.append(
                {
                    "page": pnum,
                    "image_id": f"{counter:03}",
                    "name": fname,
                    "bytes": data,
                    "width": getattr(img_obj, "width", None),
                    "height": getattr(img_obj, "height", None),
                }
            )
    return images

import json
import yaml
import unicodedata

def normalize_filename(name: str) -> str:
    return unicodedata.normalize("NFC", name)

def apply_text_replacements_from_fixmap(
    equipment_data: dict,
    fixes_files: dict[str, bytes],
    target_filename: str = "é˜²ç½è¨­å‚™ãƒãƒ³ãƒˆã‚™ãƒ•ã‚™ãƒƒã‚¯_èƒ½ç¾é˜²ç½æ ªå¼ä¼šç¤¾_å•†å“æœ¬éƒ¨_2024å¹´7æœˆ_ocræ¸ˆã¿.pdf"
) -> dict:
    """
    fixes_map.json ã«å¾“ã£ã¦ã€equipment_data ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿®æ­£ã™ã‚‹ã€‚
    ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆtarget_filenameï¼‰ã®ã¿ã«é©ç”¨ã€‚

    Args:
        equipment_data (dict): preprocess_files() ã®å‡ºåŠ›
        fixes_files (dict): download_fix_files_from_drive() ã®å‡ºåŠ›
        target_filename (str): è£œæ­£å¯¾è±¡ã¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆä¾‹: "èƒ½è¦‹é˜²ç½.pdf"ï¼‰

    Returns:
        dict: ä¿®æ­£å¾Œã® equipment_data
    """
    if "fixes_map.json" not in fixes_files:
        logger.info("âš ï¸ fixes_map.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä¿®æ­£ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return equipment_data

    logger.info(f"ğŸ”§ fixes_map.json ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    try:
        fixmap = json.loads(fixes_files["fixes_map.json"].decode("utf-8"))
    except Exception as e:
        logger.info(f"âŒ fixes_map.json ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return equipment_data

    for equipment_name, eq_data in equipment_data.items():
        for filename, original_text in eq_data["files"].items():
            if filename != target_filename:
                continue  # å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«åä»¥å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—

            logger.info(f"\nğŸ“„ ä¿®æ­£å¯¾è±¡: {equipment_name} / {filename}")
            modified_text = original_text

            for fix in fixmap:
                start_line = fix["start_line"].strip()
                end_line = fix["end_line"].strip()
                replacement_file = fix["replacement_file"]
                fix_type = fix["type"]
                description = fix.get("description", "").strip()

                normalized_target = normalize_filename(replacement_file)

                # ğŸ” fixes_files å†…ã§æ­£è¦åŒ–ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ¤œç´¢ã—ã¦å–å¾—
                replacement_filename = next(
                    (k for k in fixes_files.keys() if normalize_filename(k) == normalized_target),
                    None
                )

                # ğŸ”§ replacement_content ã®ç”Ÿæˆ
                try:
                    if fix_type == "png":
                        # ğŸ”¸ ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªãã¦ã‚‚ description ã ã‘å‡ºåŠ›
                        replacement_content = f"[ç”»åƒå‚ç…§: {replacement_file}]\n{description}"
                    else:
                        if not replacement_filename:
                            logger.warning(f"âš ï¸ replacement_file ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {replacement_file}")
                            continue

                        raw_data = fixes_files[replacement_filename]

                        if fix_type == "txt":
                            replacement_content = f"{description}\n{raw_data.decode('utf-8')}"
                        elif fix_type == "json":
                            json_content = json.dumps(json.loads(raw_data), ensure_ascii=False, indent=2)
                            replacement_content = f"{description}\n{json_content}"
                        elif fix_type == "yaml":
                            yaml_content = yaml.safe_dump(yaml.safe_load(raw_data), allow_unicode=True)
                            replacement_content = f"{description}\n{yaml_content}"
                        else:
                            logger.warning(f"âš ï¸ æœªå¯¾å¿œã® type: {fix_type}")
                            continue

                    # ğŸ“Œ å®Œå…¨ä¸€è‡´ã§è¡Œç•ªå·ã‚’å–å¾—
                    lines = modified_text.splitlines()
                    start_idx = next((i for i, line in enumerate(lines) if line.strip() == start_line), -1)
                    end_idx = next((i for i, line in enumerate(lines[start_idx + 1:], start=start_idx + 1)
                                    if line.strip() == end_line), -1)

                    if start_idx == -1 or end_idx == -1:
                        logger.info(f"âš ï¸ æŒ‡å®šã•ã‚ŒãŸè¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰: '{start_line}' ï½ '{end_line}'")
                        continue

                    # âœ¨ ç½®æ›
                    lines = lines[:start_idx] + [replacement_content] + lines[end_idx + 1:]
                    modified_text = "\n".join(lines)

                    logger.info(f"âœ… ç½®æ›å®Œäº†: '{start_line}' ï½ '{end_line}' â†’ {replacement_file}")

                except Exception as e:
                    logger.info(f"âŒ ä¿®æ­£å¤±æ•—: {replacement_file} - {e}")

            equipment_data[equipment_name]["files"][filename] = modified_text

    print(f"\nğŸ¯ ãƒ†ã‚­ã‚¹ãƒˆè£œæ­£å®Œäº†")
    return equipment_data

# ---------------------------------------------------------------------------
# 5) ãƒ¡ã‚¤ãƒ³: ãƒ•ã‚¡ã‚¤ãƒ«â†’ãƒãƒ£ãƒ³ã‚¯è¾æ›¸ãƒªã‚¹ãƒˆï¼ˆå¤§å¹…ä¿®æ­£ï¼‰
# ---------------------------------------------------------------------------
def preprocess_files(
    files: List[Dict[str, Any]]
) -> Dict[str, Dict[str, Any]]:
    """
    files: List of {"name": str, "type": mime, "data": bytes, "equipment_name": str, "equipment_category": str}
    ã‚’å—ã‘å–ã‚Šã€è¨­å‚™ã”ã¨ã«ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿æŒã—ã¦è¿”ã™ã€‚
    
    Returns:
        Dict[equipment_name, {
            "files": Dict[filename, file_text],  # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒ†ã‚­ã‚¹ãƒˆä¿æŒ
            "sources": List[str],  # ä½¿ç”¨ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒªã‚¹ãƒˆ
            "equipment_category": str,
            "total_files": int,
            "total_pages": int,
            "total_chars": int
        }]
    """
    equipment_data = {}  # è¨­å‚™åã‚’ã‚­ãƒ¼ã¨ã™ã‚‹è¾æ›¸
    
    print(f"ğŸ“š è¨­å‚™ã”ã¨ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ä¿æŒå‡¦ç†é–‹å§‹ - ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files)}")

    for f in files:
        name, mime, data = f["name"], f["type"], f["data"]
        equipment_name = f.get("equipment_name", "ä¸æ˜")
        equipment_category = f.get("equipment_category", "ãã®ä»–è¨­å‚™")
        
        print(f"ğŸ“„ å‡¦ç†ä¸­: {name} â†’ è¨­å‚™: {equipment_name}")
        
        # è¨­å‚™ãƒ‡ãƒ¼ã‚¿ã®åˆæœŸåŒ–
        if equipment_name not in equipment_data:
            equipment_data[equipment_name] = {
                "files": {},  # ãƒ•ã‚¡ã‚¤ãƒ«å â†’ ãƒ†ã‚­ã‚¹ãƒˆã®è¾æ›¸
                "sources": [],
                "equipment_category": equipment_category,
                "total_files": 0,
                "total_pages": 0,
                "total_chars": 0
            }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        file_text = ""
        file_pages = 0
        include_pages = should_include_page_numbers(name)
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
        if mime == "text/plain" or name.lower().endswith(".txt"):
            try:
                raw_text = extract_text_from_txt(data)
                file_text = f"=== ãƒ•ã‚¡ã‚¤ãƒ«: {name} ===\n{raw_text}"
                file_pages = 1  # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¯1ãƒšãƒ¼ã‚¸ã¨ã—ã¦æ‰±ã†
                print(f"  âœ… TXTãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº† - æ–‡å­—æ•°: {len(file_text)}")
            except Exception as e:
                print(f"  âŒ TXTãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
                
        # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
        elif mime == "application/pdf" or name.lower().endswith(".pdf"):
            try:
                # ãƒšãƒ¼ã‚¸åˆ¥ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                pages_data = extract_text_from_pdf_by_pages(data)
                
                # å…¨ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ï¼‰
                page_texts = [f"=== ãƒ•ã‚¡ã‚¤ãƒ«: {name} ==="]  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼
                
                for page_data in pages_data:
                    page_num = page_data["page"]
                    page_text = page_data["text"].strip()
                    
                    if page_text:  # ç©ºãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—
                        # ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’å«ã‚ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•´å½¢
                        if include_pages:
                            formatted_page = f"\n--- ãƒšãƒ¼ã‚¸ {page_num} ---\n{page_text}"
                        else:
                            # ãƒšãƒ¼ã‚¸ç•ªå·ã‚’å«ã‚ãªã„å ´åˆã¯ãã®ã¾ã¾
                            formatted_page = ""
                        page_texts.append(formatted_page)
                        file_pages += 1
                
                file_text = "\n".join(page_texts)
                print(f"  âœ… PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº† - ãƒšãƒ¼ã‚¸æ•°: {file_pages}, æ–‡å­—æ•°: {len(file_text)}")
                
            except Exception as e:
                print(f"  âŒ PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        else:
            print(f"  âš ï¸ æœªå¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {mime}")
            continue
        
        # è¨­å‚™ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã«ä¿å­˜ï¼‰
        if file_text.strip():  # ç©ºã§ãªã„å ´åˆã®ã¿è¿½åŠ 
            equipment_data[equipment_name]["files"][name] = file_text
            equipment_data[equipment_name]["sources"].append(name)
            equipment_data[equipment_name]["total_files"] += 1
            equipment_data[equipment_name]["total_pages"] += file_pages
            equipment_data[equipment_name]["total_chars"] += len(file_text)
    
    # çµæœã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›
    print(f"\nğŸ“‹ è¨­å‚™ã”ã¨ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ä¿æŒå‡¦ç†å®Œäº†")
    for equipment_name, data in equipment_data.items():
        print(f"ğŸ”§ è¨­å‚™: {equipment_name}")
        print(f"   ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {data['total_files']}")
        print(f"   ãƒšãƒ¼ã‚¸æ•°: {data['total_pages']}")
        print(f"   ç·æ–‡å­—æ•°: {data['total_chars']}")
        print(f"   ã‚½ãƒ¼ã‚¹: {', '.join(data['sources'])}")
        print()
    
    return equipment_data