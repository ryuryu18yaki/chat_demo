# src/rag_preprocess.py

from __future__ import annotations
from io import BytesIO
from typing import List, Dict, Any
import hashlib

import pdfplumber       # pip install pdfplumber
from pdfminer.high_level import extract_text  # type: ignore
from pdfminer.layout import LAParams         # type: ignore
from PIL import Image

__all__ = [
    "extract_text_from_pdf",
    "extract_text_from_txt",
    "chunk_text",
    "extract_tables_from_pdf",
    "extract_images_from_pdf",
    "preprocess_files",
]

# ---------------------------------------------------------------------------
# 1) ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
# ---------------------------------------------------------------------------
def extract_text_from_pdf(data: bytes) -> str:
    """PDF ãƒã‚¤ãƒŠãƒªã‹ã‚‰å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ã€‚"""
    with BytesIO(data) as buf:
        laparams = LAParams()
        text = extract_text(buf, laparams=laparams)
    return text

def extract_text_from_txt(data: bytes, encoding: str | None = None) -> str:
    """TXT ãƒã‚¤ãƒŠãƒªã‚’æ–‡å­—åˆ—ã¸ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã€‚"""
    if encoding is None:
        for enc in ("utf-8", "shift_jis", "latin-1"):
            try:
                return data.decode(enc)
            except UnicodeDecodeError:
                continue
        raise UnicodeDecodeError("Failed to decode text file with common encodings")
    return data.decode(encoding)

# ğŸ”¥ ä¿®æ­£ç‰ˆ: ãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
def extract_text_from_pdf_by_pages(data: bytes) -> List[Dict[str, Any]]:
    """PDFã‹ã‚‰ãƒšãƒ¼ã‚¸åˆ¥ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    pages_text = []
    with pdfplumber.open(BytesIO(data)) as pdf:
        for page_num, page in enumerate(pdf.pages, start=1):
            page_text = page.extract_text() or ""
            if page_text.strip():  # ç©ºãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—
                pages_text.append({
                    "text": page_text,
                    "page": page_num
                })
    return pages_text

# ---------------------------------------------------------------------------
# 2) ãƒãƒ£ãƒ³ã‚¯åŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆä¿®æ­£ç‰ˆï¼‰
# ---------------------------------------------------------------------------
def chunk_text(text: str, *, chunk_size: int = 800, overlap: int = 80) -> List[str]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’é‡è¤‡ä»˜ãã§åˆ†å‰²ã™ã‚‹ã€‚ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’10%ã«èª¿æ•´ï¼‰"""
    if chunk_size <= overlap:
        raise ValueError("chunk_size must be larger than overlap")
    
    # ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚„çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆ
    if not text.strip() or len(text) <= chunk_size:
        return [text] if text.strip() else []
    
    chunks: List[str] = []
    start = 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunk = text[start:end].strip()
        
        if chunk:  # ç©ºã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—
            chunks.append(chunk)
        
        if end == len(text):
            break
        start = end - overlap
    
    return chunks

# ğŸ”¥ æ–°æ©Ÿèƒ½: ãƒ¦ãƒ‹ãƒ¼ã‚¯IDç”Ÿæˆ
def generate_chunk_id(source: str, page: int, chunk_index: int, content: str) -> str:
    """ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒãƒ£ãƒ³ã‚¯IDã‚’ç”Ÿæˆ"""
    content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
    return f"{source}_p{page}_{chunk_index}_{content_hash}"

# ---------------------------------------------------------------------------
# 3) è¡¨ã®æŠ½å‡º
# ---------------------------------------------------------------------------
def extract_tables_from_pdf(data: bytes) -> List[Dict[str, Any]]:
    """pdfplumber ã§è¡¨ã‚’æŠ½å‡ºã—ã€CSV ãƒ©ã‚¤ã‚¯ãªæ–‡å­—åˆ—ã§è¿”ã™ã€‚"""
    tables: List[Dict[str, Any]] = []
    with pdfplumber.open(BytesIO(data)) as pdf:
        for page_num, page in enumerate(pdf.pages, start=1):
            for tbl_idx, table in enumerate(page.extract_tables(), start=1):
                # None ã‚’ "" ã«ç½®ãæ›ãˆã¦ã‹ã‚‰çµåˆ
                lines: list[str] = []
                for row in table:
                    cells = [(cell if cell is not None else "") for cell in row]
                    lines.append(",".join(cells))
                csv_text = "\n".join(lines)
                
                if csv_text.strip():  # ç©ºã®è¡¨ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    tables.append({
                        "text": csv_text,
                        "page": page_num,
                        "table_id": tbl_idx,
                    })
    return tables

# ---------------------------------------------------------------------------
# 4) ç”»åƒã®æŠ½å‡º
# ---------------------------------------------------------------------------
def extract_images_from_pdf(data: bytes) -> List[Dict[str, Any]]:
    """pdfplumber ã§åŸ‹ã‚è¾¼ã¿ç”»åƒã‚’æŠ½å‡ºã—ã€ãƒã‚¤ãƒˆï¼‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§è¿”ã™ã€‚"""
    imgs: List[Dict[str, Any]] = []
    with pdfplumber.open(BytesIO(data)) as pdf:
        for page_num, page in enumerate(pdf.pages, start=1):
            # â‘  ãƒšãƒ¼ã‚¸å…¨ä½“ã‚’ PIL Image ã¨ã—ã¦å–å¾—
            page_img = page.to_image(resolution=150)
            pil_page = page_img.original
            for img_idx, img_meta in enumerate(page.images, start=1):
                x0, top, x1, bottom = (
                    img_meta["x0"], img_meta["top"],
                    img_meta["x1"], img_meta["bottom"]
                )
                # â‘¡ PIL Image ã«å¯¾ã—ã¦ crop
                cropped = pil_page.crop((x0, top, x1, bottom))
                # PIL ã§ PNG ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                buf = BytesIO()
                cropped.save(buf, format="PNG")
                img_bytes = buf.getvalue()
                imgs.append({
                    "bytes": img_bytes,
                    "page": page_num,
                    "image_id": img_idx,
                    "width": cropped.width,
                    "height": cropped.height,
                })
    return imgs

# ---------------------------------------------------------------------------
# 5) ãƒ¡ã‚¤ãƒ³: ãƒ•ã‚¡ã‚¤ãƒ«â†’ãƒãƒ£ãƒ³ã‚¯è¾æ›¸ãƒªã‚¹ãƒˆï¼ˆå¤§å¹…ä¿®æ­£ï¼‰
# ---------------------------------------------------------------------------
def preprocess_files(
    files: List[Dict[str, Any]],
    *,
    chunk_size: int = 800,
    overlap: int = 80,  # 10%ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
) -> List[Dict[str, Any]]:
    """
    files: List of {"name": str, "type": mime, "data": bytes}
    ã‚’å—ã‘å–ã‚Šã€kind(text/table/image)ã”ã¨ã« docs ã‚’è¿”ã™ã€‚
    
    ğŸ”¥ ä¿®æ­£ç‚¹:
    - ãƒšãƒ¼ã‚¸åˆ¥å‡¦ç†ã§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®é‡è¤‡ã‚’é˜²ã
    - ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã®ç”Ÿæˆ
    - é‡è¤‡ãƒãƒ£ãƒ³ã‚¯ã®é™¤å»
    """
    docs: List[Dict[str, Any]] = []
    seen_content = set()  # é‡è¤‡é™¤å»ç”¨

    for f in files:
        name, mime, data = f["name"], f["type"], f["data"]
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        file_hash = hashlib.md5(data).hexdigest()[:8]

        # ğŸ”¥ è¡¨ã®ãƒãƒ£ãƒ³ã‚¯åŒ–ï¼ˆä¿®æ­£ï¼‰
        if name.lower().endswith(".pdf"):
            tables = extract_tables_from_pdf(data)
            for tbl in tables:
                content = tbl["text"]
                content_hash = hashlib.md5(content.encode()).hexdigest()
                
                if content_hash not in seen_content:
                    unique_id = f"{name}_p{tbl['page']}_table{tbl['table_id']}_{content_hash[:8]}"
                    docs.append({
                        "content": content,
                        "metadata": {
                            "source": name,
                            "kind": "table",
                            "page": tbl["page"],
                            "table_id": tbl["table_id"],
                            "unique_id": unique_id,
                            "file_hash": file_hash,
                        }
                    })
                    seen_content.add(content_hash)

        # ğŸ”¥ ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒ£ãƒ³ã‚¯åŒ–ï¼ˆãƒšãƒ¼ã‚¸åˆ¥å‡¦ç†ï¼‰
        if mime == "text/plain" or name.lower().endswith(".txt"):
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆï¼ˆãƒšãƒ¼ã‚¸ãªã—ï¼‰
            text = extract_text_from_txt(data)
            if text.strip():
                for chunk_idx, chunk in enumerate(chunk_text(text, chunk_size=chunk_size, overlap=overlap)):
                    content_hash = hashlib.md5(chunk.encode()).hexdigest()
                    
                    if content_hash not in seen_content:
                        unique_id = generate_chunk_id(name, 1, chunk_idx, chunk)
                        docs.append({
                            "content": chunk,
                            "metadata": {
                                "source": name,
                                "kind": "text",
                                "page": 1,  # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¯1ãƒšãƒ¼ã‚¸ã¨ã—ã¦æ‰±ã†
                                "chunk_id": chunk_idx,
                                "unique_id": unique_id,
                                "file_hash": file_hash,
                            }
                        })
                        seen_content.add(content_hash)
                        
        elif mime == "application/pdf" or name.lower().endswith(".pdf"):
            # ğŸ”¥ PDFã®å ´åˆï¼šãƒšãƒ¼ã‚¸åˆ¥å‡¦ç†
            pages_data = extract_text_from_pdf_by_pages(data)
            
            for page_data in pages_data:
                page_num = page_data["page"]
                page_text = page_data["text"]
                
                if page_text.strip():  # ç©ºãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    chunks = chunk_text(page_text, chunk_size=chunk_size, overlap=overlap)
                    
                    for chunk_idx, chunk in enumerate(chunks):
                        content_hash = hashlib.md5(chunk.encode()).hexdigest()
                        
                        if content_hash not in seen_content:
                            unique_id = generate_chunk_id(name, page_num, chunk_idx, chunk)
                            docs.append({
                                "content": chunk,
                                "metadata": {
                                    "source": name,
                                    "kind": "text",
                                    "page": page_num,  # ğŸ”¥ æ­£ç¢ºãªãƒšãƒ¼ã‚¸ç•ªå·
                                    "chunk_id": chunk_idx,
                                    "unique_id": unique_id,
                                    "file_hash": file_hash,
                                }
                            })
                            seen_content.add(content_hash)

    return docs