# src/rag_preprocess.py - ãƒ†ãƒ¼ãƒ–ãƒ«å¯¾å¿œç‰ˆï¼ˆé‡è¤‡ã¯ãã®ã¾ã¾ï¼‰

from __future__ import annotations
from io import BytesIO
from typing import List, Dict, Any
import hashlib

import pdfplumber       # pip install pdfplumber
from pdfminer.high_level import extract_text  # type: ignore
from pdfminer.layout import LAParams         # type: ignore
from pypdf import PdfReader
from PIL import Image

__all__ = [
    "extract_text_from_pdf",
    "extract_text_from_txt", 
    "chunk_text",
    "extract_tables_from_pdf",
    "extract_images_from_pdf",
    "preprocess_files",
    "format_table_as_text",
    "extract_structured_content"
]

# ---------------------------------------------------------------------------
# 1) ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆæ—¢å­˜ï¼‰
# ---------------------------------------------------------------------------
def extract_text_from_pdf(data: bytes) -> str:
    """PDF ãƒã‚¤ãƒŠãƒªã‹ã‚‰å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ã€‚"""
    with BytesIO(data) as buf:
        laparams = LAParams()
        text = extract_text(buf, laparams=laparams)
    return text

def extract_text_from_txt(data: bytes, encoding: str | None = None) -> str:
    """TXT ãƒã‚¤ãƒŠãƒªã‚’æ–‡å­—åˆ—ã¸ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã€‚"""
    if encoding is None:
        for enc in ("utf-8", "shift_jis", "latin-1"):
            try:
                return data.decode(enc)
            except UnicodeDecodeError:
                continue
        raise UnicodeDecodeError("Failed to decode text file with common encodings")
    return data.decode(encoding)

def extract_text_from_pdf_by_pages(data: bytes) -> List[Dict[str, Any]]:
    """PDFã‹ã‚‰ãƒšãƒ¼ã‚¸åˆ¥ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    pages_text = []
    with pdfplumber.open(BytesIO(data)) as pdf:
        for page_num, page in enumerate(pdf.pages, start=1):
            page_text = page.extract_text() or ""
            if page_text.strip():  # ç©ºãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—
                pages_text.append({
                    "text": page_text,
                    "page": page_num
                })
    return pages_text

# ---------------------------------------------------------------------------
# 2) ãƒãƒ£ãƒ³ã‚¯åŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆæ—¢å­˜ï¼‰
# ---------------------------------------------------------------------------
def chunk_text(text: str, *, chunk_size: int = 800, overlap: int = 80) -> List[str]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’é‡è¤‡ä»˜ãã§åˆ†å‰²ã™ã‚‹ã€‚ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’10%ã«èª¿æ•´ï¼‰"""
    if chunk_size <= overlap:
        raise ValueError("chunk_size must be larger than overlap")
    
    # ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚„çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆ
    if not text.strip() or len(text) <= chunk_size:
        return [text] if text.strip() else []
    
    chunks: List[str] = []
    start = 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunk = text[start:end].strip()
        
        if chunk:  # ç©ºã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—
            chunks.append(chunk)
        
        if end == len(text):
            break
        start = end - overlap
    
    return chunks

def generate_chunk_id(source: str, page: int, chunk_index: int, content: str) -> str:
    """ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒãƒ£ãƒ³ã‚¯IDã‚’ç”Ÿæˆ"""
    content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
    return f"{source}_p{page}_{chunk_index}_{content_hash}"

# ---------------------------------------------------------------------------
# 3) è¡¨ã®æŠ½å‡ºï¼ˆå¼·åŒ–ç‰ˆï¼‰
# ---------------------------------------------------------------------------
def extract_tables_from_pdf(data: bytes) -> List[Dict[str, Any]]:
    """pdfplumber ã§è¡¨ã‚’æŠ½å‡ºã—ã€æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦è¿”ã™ã€‚"""
    tables: List[Dict[str, Any]] = []
    with pdfplumber.open(BytesIO(data)) as pdf:
        for page_num, page in enumerate(pdf.pages, start=1):
            page_tables = page.extract_tables()
            
            for tbl_idx, table in enumerate(page_tables, start=1):
                if not table or len(table) == 0:
                    continue
                    
                # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
                processed_table = []
                headers = []
                
                # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®æ¤œå‡ºï¼ˆæœ€åˆã®è¡Œã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦æ‰±ã†ï¼‰
                if table[0]:
                    headers = [str(cell).strip() if cell else f"åˆ—{i+1}" for i, cell in enumerate(table[0])]
                
                # ãƒ‡ãƒ¼ã‚¿è¡Œã®å‡¦ç†
                for row_idx, row in enumerate(table[1:] if headers else table, start=1):
                    if not row:
                        continue
                    processed_row = [str(cell).strip() if cell else "" for cell in row]
                    if any(processed_row):  # ç©ºè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                        processed_table.append(processed_row)
                
                if processed_table:
                    # CSVå½¢å¼ã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
                    csv_lines = []
                    if headers:
                        csv_lines.append(",".join(f'"{h}"' for h in headers))
                    
                    for row in processed_table:
                        csv_lines.append(",".join(f'"{cell}"' for cell in row))
                    
                    csv_text = "\n".join(csv_lines)
                    
                    # æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±
                    table_info = {
                        "page": page_num,
                        "table_id": tbl_idx,
                        "headers": headers,
                        "data": processed_table,
                        "csv_text": csv_text,
                        "row_count": len(processed_table),
                        "col_count": len(headers) if headers else (len(processed_table[0]) if processed_table else 0),
                        "formatted_text": format_table_as_text(headers, processed_table)
                    }
                    
                    tables.append(table_info)
                    print(f"ğŸ“Š ãƒ†ãƒ¼ãƒ–ãƒ«æŠ½å‡º: ãƒšãƒ¼ã‚¸{page_num}, ãƒ†ãƒ¼ãƒ–ãƒ«{tbl_idx} - {len(processed_table)}è¡ŒÃ—{table_info['col_count']}åˆ—")
    
    return tables

def format_table_as_text(headers: List[str], data: List[List[str]]) -> str:
    """ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿ã‚„ã™ã„ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã«å¤‰æ›"""
    if not data:
        return ""
    
    lines = []
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±
    if headers:
        lines.append("ã€ãƒ†ãƒ¼ãƒ–ãƒ«é …ç›®ã€‘")
        for i, header in enumerate(headers):
            lines.append(f"  {i+1}. {header}")
        lines.append("")
    
    # ãƒ‡ãƒ¼ã‚¿è¡Œã®å‡¦ç†
    lines.append("ã€ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã€‘")
    for row_idx, row in enumerate(data, start=1):
        lines.append(f"è¡Œ{row_idx}:")
        
        if headers and len(row) == len(headers):
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã¨å¯¾å¿œä»˜ã‘ã¦è¡¨ç¤º
            for header, value in zip(headers, row):
                if value.strip():  # ç©ºã§ãªã„å€¤ã®ã¿
                    lines.append(f"  ãƒ»{header}: {value}")
        else:
            # ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã¾ãŸã¯åˆ—æ•°ä¸ä¸€è‡´ã®å ´åˆ
            for col_idx, value in enumerate(row):
                if value.strip():
                    lines.append(f"  ãƒ»åˆ—{col_idx+1}: {value}")
        lines.append("")
    
    return "\n".join(lines)

# ---------------------------------------------------------------------------
# 4) ç”»åƒã®æŠ½å‡ºï¼ˆæ—¢å­˜ï¼‰
# ---------------------------------------------------------------------------
def extract_images_from_pdf(pdf_bytes: bytes) -> List[Dict[str, Any]]:
    reader = PdfReader(BytesIO(pdf_bytes))
    images, counter = [], 0

    for pnum, page in enumerate(reader.pages, start=1):
        for img_obj in page.images:
            counter += 1
            data = img_obj.data

            # --- Pillow ã§å½¢å¼åˆ¤å®š ---
            try:
                fmt = Image.open(BytesIO(data)).format.lower()  # 'jpeg', 'png', ...
            except Exception:
                fmt = "png"  # ä½•ã‹ã‚ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            ext = "jpg" if fmt == "jpeg" else fmt

            fname = f"page{pnum}_{counter:03}.{ext}"
            images.append(
                {
                    "page": pnum,
                    "image_id": f"{counter:03}",
                    "name": fname,
                    "bytes": data,
                    "width": getattr(img_obj, "width", None),
                    "height": getattr(img_obj, "height", None),
                }
            )
    return images

# ---------------------------------------------------------------------------
# 5) æ§‹é€ åŒ–ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
# ---------------------------------------------------------------------------
def extract_structured_content(data: bytes, filename: str) -> Dict[str, Any]:
    """PDFã‹ã‚‰æ§‹é€ åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆãƒ†ã‚­ã‚¹ãƒˆ+ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰ã‚’æŠ½å‡º"""
    
    result = {
        "text_content": [],
        "table_content": [],
        "total_text_chars": 0,
        "total_tables": 0
    }
    
    # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
    try:
        pages_text = extract_text_from_pdf_by_pages(data)
        result["text_content"] = pages_text
        result["total_text_chars"] = sum(len(page["text"]) for page in pages_text)
        print(f"ğŸ“„ ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†: {filename} - {len(pages_text)}ãƒšãƒ¼ã‚¸, {result['total_text_chars']}æ–‡å­—")
    except Exception as e:
        print(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {filename} - {e}")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«æŠ½å‡º
    try:
        tables = extract_tables_from_pdf(data)
        result["table_content"] = tables
        result["total_tables"] = len(tables)
        print(f"ğŸ“Š ãƒ†ãƒ¼ãƒ–ãƒ«æŠ½å‡ºå®Œäº†: {filename} - {len(tables)}ãƒ†ãƒ¼ãƒ–ãƒ«")
    except Exception as e:
        print(f"âŒ ãƒ†ãƒ¼ãƒ–ãƒ«æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {filename} - {e}")
    
    return result

# ---------------------------------------------------------------------------
# 6) ãƒ¡ã‚¤ãƒ³: ãƒ•ã‚¡ã‚¤ãƒ«â†’è¨­å‚™è¾æ›¸ãƒªã‚¹ãƒˆï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«å¯¾å¿œç‰ˆï¼‰
# ---------------------------------------------------------------------------
def preprocess_files(
    files: List[Dict[str, Any]]
) -> Dict[str, Dict[str, Any]]:
    """
    files: List of {"name": str, "type": mime, "data": bytes, "equipment_name": str, "equipment_category": str}
    ã‚’å—ã‘å–ã‚Šã€è¨­å‚™ã”ã¨ã«ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã§ãƒ†ã‚­ã‚¹ãƒˆ+ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä¿æŒã—ã¦è¿”ã™ã€‚
    
    Returns:
        Dict[equipment_name, {
            "files": Dict[filename, file_content],  # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä¿æŒ
            "sources": List[str],  # ä½¿ç”¨ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒªã‚¹ãƒˆ
            "equipment_category": str,
            "total_files": int,
            "total_pages": int,
            "total_chars": int,
            "total_tables": int,  # ãƒ†ãƒ¼ãƒ–ãƒ«æ•°
            "table_info": List[Dict]  # ãƒ†ãƒ¼ãƒ–ãƒ«è©³ç´°æƒ…å ±
        }]
    """
    equipment_data = {}  # è¨­å‚™åã‚’ã‚­ãƒ¼ã¨ã™ã‚‹è¾æ›¸
    
    print(f"ğŸ“š è¨­å‚™ã”ã¨ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ä¿æŒå‡¦ç†é–‹å§‹ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«å¯¾å¿œç‰ˆï¼‰ - ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files)}")

    for f in files:
        name, mime, data = f["name"], f["type"], f["data"]
        equipment_name = f.get("equipment_name", "ä¸æ˜")
        equipment_category = f.get("equipment_category", "ãã®ä»–è¨­å‚™")
        
        print(f"ğŸ“„ å‡¦ç†ä¸­: {name} â†’ è¨­å‚™: {equipment_name}")
        
        # è¨­å‚™ãƒ‡ãƒ¼ã‚¿ã®åˆæœŸåŒ–
        if equipment_name not in equipment_data:
            equipment_data[equipment_name] = {
                "files": {},  # ãƒ•ã‚¡ã‚¤ãƒ«å â†’ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è¾æ›¸
                "sources": [],
                "equipment_category": equipment_category,
                "total_files": 0,
                "total_pages": 0,
                "total_chars": 0,
                "total_tables": 0,  # ãƒ†ãƒ¼ãƒ–ãƒ«æ•°
                "table_info": []    # ãƒ†ãƒ¼ãƒ–ãƒ«è©³ç´°æƒ…å ±
            }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
        file_content = ""
        file_pages = 0
        file_tables = 0
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
        if mime == "text/plain" or name.lower().endswith(".txt"):
            try:
                raw_text = extract_text_from_txt(data)
                file_content = f"=== ãƒ•ã‚¡ã‚¤ãƒ«: {name} ===\n{raw_text}"
                file_pages = 1  # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¯1ãƒšãƒ¼ã‚¸ã¨ã—ã¦æ‰±ã†
                print(f"  âœ… TXTãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº† - æ–‡å­—æ•°: {len(file_content)}")
            except Exception as e:
                print(f"  âŒ TXTãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
                
        # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«å¯¾å¿œç‰ˆï¼‰
        elif mime == "application/pdf" or name.lower().endswith(".pdf"):
            try:
                # æ§‹é€ åŒ–ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
                structured = extract_structured_content(data, name)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼
                content_parts = [f"=== ãƒ•ã‚¡ã‚¤ãƒ«: {name} ==="]
                
                # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å‡¦ç†
                for page_data in structured["text_content"]:
                    page_num = page_data["page"]
                    page_text = page_data["text"].strip()
                    
                    if page_text:  # ç©ºãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—
                        formatted_page = f"\n--- ãƒšãƒ¼ã‚¸ {page_num} ---\n{page_text}"
                        content_parts.append(formatted_page)
                        file_pages += 1
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å‡¦ç†
                if structured["table_content"]:
                    content_parts.append(f"\n=== ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ± ({len(structured['table_content'])}å€‹) ===")
                    
                    for table in structured["table_content"]:
                        table_header = f"\n--- ãƒšãƒ¼ã‚¸{table['page']} ãƒ†ãƒ¼ãƒ–ãƒ«{table['table_id']} ({table['row_count']}è¡ŒÃ—{table['col_count']}åˆ—) ---"
                        content_parts.append(table_header)
                        content_parts.append(table["formatted_text"])
                        
                        # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’è¨­å‚™ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
                        table_info = {
                            "source_file": name,
                            "page": table["page"],
                            "table_id": table["table_id"],
                            "headers": table["headers"],
                            "row_count": table["row_count"],
                            "col_count": table["col_count"],
                            "formatted_text": table["formatted_text"]
                        }
                        equipment_data[equipment_name]["table_info"].append(table_info)
                        file_tables += 1
                
                file_content = "\n".join(content_parts)
                
                print(f"  âœ… PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº† - ãƒšãƒ¼ã‚¸æ•°: {file_pages}, ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {file_tables}, æ–‡å­—æ•°: {len(file_content)}")
                
            except Exception as e:
                print(f"  âŒ PDFãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        else:
            print(f"  âš ï¸ æœªå¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {mime}")
            continue
        
        # è¨­å‚™ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã«ä¿å­˜ï¼‰
        if file_content.strip():  # ç©ºã§ãªã„å ´åˆã®ã¿è¿½åŠ 
            equipment_data[equipment_name]["files"][name] = file_content
            equipment_data[equipment_name]["sources"].append(name)
            equipment_data[equipment_name]["total_files"] += 1
            equipment_data[equipment_name]["total_pages"] += file_pages
            equipment_data[equipment_name]["total_chars"] += len(file_content)
            equipment_data[equipment_name]["total_tables"] += file_tables
    
    # çµæœã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›
    print(f"\nğŸ“‹ è¨­å‚™ã”ã¨ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ä¿æŒå‡¦ç†å®Œäº†ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«å¯¾å¿œç‰ˆï¼‰")
    for equipment_name, data in equipment_data.items():
        print(f"ğŸ”§ è¨­å‚™: {equipment_name}")
        print(f"   ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {data['total_files']}")
        print(f"   ãƒšãƒ¼ã‚¸æ•°: {data['total_pages']}")
        print(f"   ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {data['total_tables']}")
        print(f"   ç·æ–‡å­—æ•°: {data['total_chars']}")
        print(f"   ã‚½ãƒ¼ã‚¹: {', '.join(data['sources'])}")
        print()
    
    return equipment_data