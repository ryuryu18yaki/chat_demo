# src/rag_qa.py - ãƒ†ãƒ¼ãƒ–ãƒ«å¯¾å¿œç‰ˆï¼ˆAæ¡ˆï¼šé¸æŠãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ†ãƒ¼ãƒ–ãƒ«å¸¸ã«ä½¿ç”¨ï¼‰

from __future__ import annotations
from typing import List, Dict, Any, Optional
from openai import AzureOpenAI
from base64 import b64encode
import os

try:
    import streamlit as st
    STREAMLIT_AVAILABLE = True
except ImportError:
    STREAMLIT_AVAILABLE = False

# ---------------------------------------------------------------------------
# Azure OpenAIè¨­å®šï¼ˆæ—¢å­˜ï¼‰
# ---------------------------------------------------------------------------
def create_azure_openai_client():
    """Azure OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ"""
    if STREAMLIT_AVAILABLE:
        try:
            azure_endpoint = st.secrets.get("AZURE_OPENAI_ENDPOINT", os.getenv("AZURE_OPENAI_ENDPOINT"))
            azure_key = st.secrets.get("AZURE_OPENAI_KEY", os.getenv("AZURE_OPENAI_KEY"))
        except:
            azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
            azure_key = os.getenv("AZURE_OPENAI_KEY")
    else:
        azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
        azure_key = os.getenv("AZURE_OPENAI_KEY")
    
    if not azure_endpoint or not azure_key:
        raise ValueError("Azure OpenAI ã®è¨­å®šãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    
    return AzureOpenAI(
        api_version="2025-04-01-preview",
        azure_endpoint=azure_endpoint,
        api_key=azure_key
    )

# Azureç”¨ã®ãƒ¢ãƒ‡ãƒ«åãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ—¢å­˜ï¼‰
AZURE_MODEL_MAPPING = {
    "gpt-4.1": "gpt-4.1",
    "gpt-4.1-mini": "gpt-4.1-mini", 
    "gpt-4.1-nano": "gpt-4.1-nano",
    "gpt-4o": "gpt-4o",
    "gpt-4o-mini": "gpt-4o-mini"
}

def get_azure_model_name(model_name: str) -> str:
    """OpenAIãƒ¢ãƒ‡ãƒ«åã‚’Azureãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆåã«å¤‰æ›"""
    return AZURE_MODEL_MAPPING.get(model_name, model_name)

# ---------------------------------------------------------------------------
# è¨­å®šï¼ˆæ—¢å­˜ï¼‰
# ---------------------------------------------------------------------------
_DEFAULT_MODEL = "gpt-4o-mini"

# ---------------------------------------------------------------------------
# å›ç­”ç”Ÿæˆï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«å¯¾å¿œç‰ˆ - Aæ¡ˆå®Ÿè£…ï¼‰
# ---------------------------------------------------------------------------
def generate_answer_with_equipment(
        *,
        prompt: str,
        question: str,
        equipment_data: Dict[str, Dict[str, Any]],
        target_equipment: str,
        selected_files: Optional[List[str]] = None,
        model: str = _DEFAULT_MODEL,
        chat_history: Optional[List[Dict[str, str]]] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
    ) -> Dict[str, Any]:
    """
    æŒ‡å®šã•ã‚ŒãŸè¨­å‚™ã®é¸æŠãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æŠ•å…¥ã—ã¦å›ç­”ã‚’ç”Ÿæˆï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«å¯¾å¿œç‰ˆ - Aæ¡ˆï¼‰
    
    Aæ¡ˆ: é¸æŠãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã¯å¸¸ã«ä½¿ç”¨
    """
    # Azure OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
    client = create_azure_openai_client()

    # --- 1) æŒ‡å®šè¨­å‚™ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾— ---
    if target_equipment not in equipment_data:
        available_equipment = list(equipment_data.keys())
        raise ValueError(f"è¨­å‚™ '{target_equipment}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ©ç”¨å¯èƒ½ãªè¨­å‚™: {available_equipment}")
    
    equipment_info = equipment_data[target_equipment]
    available_files = equipment_info["files"]  # ãƒ•ã‚¡ã‚¤ãƒ«å â†’ ãƒ†ã‚­ã‚¹ãƒˆã®è¾æ›¸
    all_sources = equipment_info["sources"]
    table_info = equipment_info.get("table_info", [])  # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±
    
    # --- 2) é¸æŠã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’çµåˆ ---
    if selected_files is not None:
        print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {target_equipment}")
        print(f"ğŸ“„ é¸æŠãƒ•ã‚¡ã‚¤ãƒ«: {', '.join(selected_files)}")
        
        # é¸æŠã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
        selected_texts = []
        actual_sources = []
        
        for file_name in selected_files:
            if file_name in available_files:
                selected_texts.append(available_files[file_name])
                actual_sources.append(file_name)
            else:
                print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_name}")
        
        if not selected_texts:
            raise ValueError(f"é¸æŠã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ{', '.join(selected_files)}ï¼‰ãŒè¨­å‚™ãƒ‡ãƒ¼ã‚¿ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        
        combined_text = "\n\n".join(selected_texts)
        sources = actual_sources
        
        # Aæ¡ˆ: é¸æŠãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’å¸¸ã«å–å¾—
        relevant_table_info = [t for t in table_info if t['source_file'] in selected_files]
        
    else:
        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨
        selected_texts = list(available_files.values())
        combined_text = "\n\n".join(selected_texts)
        sources = all_sources
        relevant_table_info = table_info
    
    # --- 3) ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®å‡¦ç†ï¼ˆAæ¡ˆï¼šå¸¸ã«ä½¿ç”¨ï¼‰ ---
    table_context = ""
    used_tables = []
    
    if relevant_table_info:
        print(f"ğŸ“Š Aæ¡ˆé©ç”¨: é¸æŠãƒ•ã‚¡ã‚¤ãƒ«å†…ã®å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½¿ç”¨ ({len(relevant_table_info)}å€‹)")
        
        table_context += "\n\nã€é‡è¦: ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»è¡¨ãƒ‡ãƒ¼ã‚¿ã€‘\n"
        table_context += f"é¸æŠãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ± (å…¨{len(relevant_table_info)}å€‹):\n\n"
        
        for i, table in enumerate(relevant_table_info, 1):
            table_header = f"â–  ãƒ†ãƒ¼ãƒ–ãƒ«{i}: {table['source_file']} ãƒšãƒ¼ã‚¸{table['page']}"
            table_header += f" ({table['row_count']}è¡ŒÃ—{table['col_count']}åˆ—)"
            
            table_context += f"{table_header}\n"
            if table.get('headers'):
                table_context += f"åˆ—ãƒ˜ãƒƒãƒ€ãƒ¼: {' | '.join(table['headers'])}\n"
            table_context += f"{table['formatted_text']}\n"
            table_context += "-" * 60 + "\n\n"
            
            used_tables.append({
                "source_file": table['source_file'],
                "page": table['page'],
                "table_id": table['table_id'],
                "usage_mode": "Aæ¡ˆ_å¸¸ã«ä½¿ç”¨"
            })
        
        print(f"ğŸ“Š ä½¿ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(used_tables)}")
    else:
        print("ğŸ“Š é¸æŠãƒ•ã‚¡ã‚¤ãƒ«å†…ã«ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“")
    
    # --- 4) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ„ã¿ç«‹ã¦ ---
    equipment_context = f"""
ã€å‚è€ƒè³‡æ–™ã€‘è¨­å‚™: {target_equipment} (ã‚«ãƒ†ã‚´ãƒª: {equipment_info['equipment_category']})
ä½¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ«: {', '.join(sources)}
ä½¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(sources)}/{len(all_sources)}
ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(used_tables)}å€‹ï¼ˆé¸æŠãƒ•ã‚¡ã‚¤ãƒ«å†…: {len(relevant_table_info)}å€‹, ç·ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(table_info)}å€‹ï¼‰

ã€è³‡æ–™å†…å®¹ã€‘
{combined_text}
{table_context}
"""
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ä½¿ç”¨æ™‚ã®è¿½åŠ æŒ‡ç¤º
    table_instruction = ""
    if used_tables:
        table_instruction = """

ã€ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®æ´»ç”¨ã«ã¤ã„ã¦ã€‘
- ä¸Šè¨˜ã®ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»è¡¨ãƒ‡ãƒ¼ã‚¿ã¯é¸æŠã•ã‚ŒãŸè³‡æ–™ã«å«ã¾ã‚Œã‚‹é‡è¦ãªæ§‹é€ åŒ–æƒ…å ±ã§ã™
- æ•°å€¤ã‚„åŸºæº–ã€æ¡ä»¶ç­‰ã‚’å›ç­”ã™ã‚‹éš›ã¯ã€ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’å„ªå…ˆçš„ã«å‚ç…§ã—ã¦ãã ã•ã„
- ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡Œãƒ»åˆ—æƒ…å ±ã‚’æ­£ç¢ºã«èª­ã¿å–ã‚Šã€è©²å½“ã™ã‚‹é …ç›®ã‚’ç‰¹å®šã—ã¦ãã ã•ã„
- å¯èƒ½ãªé™ã‚Šå…·ä½“çš„ãªæ•°å€¤ã‚„æ¡ä»¶ã‚’ç¤ºã—ã¦ãã ã•ã„
- ã€Œâ—‹ã€ã€ŒÃ—ã€ã€Œâ—ã€ãªã©ã®è¨˜å·ã‚„ã€é¢ç©ãƒ»éšæ•°ãªã©ã®æ•°å€¤æ¡ä»¶ã«ç‰¹ã«æ³¨æ„ã—ã¦ãã ã•ã„
"""
    
    system_msg = {
        "role": "system",
        "content": prompt + table_instruction
    }
    
    user_msg = {
        "role": "user", 
        "content": f"{equipment_context}\n\nã€è³ªå•ã€‘\n{question}\n\nä¸Šè¨˜ã®è³‡æ–™ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±å«ã‚€ï¼‰ã‚’å‚è€ƒã«ã€æ—¥æœ¬èªã§è©³ç´°ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    }
    
    # --- 5) Messages çµ„ã¿ç«‹ã¦ ---
    messages: List[Dict[str, Any]] = []
    
    if chat_history:
        messages.append(system_msg)
        safe_history = [
            {"role": m.get("role"), "content": m.get("content")}
            for m in chat_history
            if isinstance(m, dict) and m.get("role") and m.get("content")
        ]
        messages.extend(safe_history[:-1])
        messages.append(user_msg)
    else:
        messages = [system_msg, user_msg]
    
    # --- 6) APIå‘¼ã³å‡ºã—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰ ---
    params = {
        "model": get_azure_model_name(model),
        "messages": messages,
    }
    
    if temperature is not None:
        params["temperature"] = temperature
    if max_tokens is not None:
        params["max_tokens"] = max_tokens
    
    # --- 7) Azure OpenAI å‘¼ã³å‡ºã— ---
    try:
        print(f"ğŸ¤– APIå‘¼ã³å‡ºã—é–‹å§‹ - ãƒ¢ãƒ‡ãƒ«: {get_azure_model_name(model)}")
        resp = client.chat.completions.create(**params)
        answer = resp.choices[0].message.content
        print(f"âœ… å›ç­”ç”Ÿæˆå®Œäº† - å›ç­”æ–‡å­—æ•°: {len(answer)}")
        
    except Exception as e:
        print(f"âŒ APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
        raise
    
    return {
        "answer": answer,
        "used_equipment": target_equipment,
        "equipment_info": equipment_info,
        "sources": sources,
        "selected_files": selected_files,
        "context_length": len(combined_text),
        "used_tables": used_tables,
        "total_tables": len(table_info),
        "relevant_tables_in_files": len(relevant_table_info),
        "table_usage_mode": "Aæ¡ˆ_å¸¸ã«ä½¿ç”¨",
        "images": []
    }

# ---------------------------------------------------------------------------
# è³ªå•ã‹ã‚‰è¨­å‚™ã‚’è‡ªå‹•æ¨å®šã™ã‚‹é–¢æ•°ï¼ˆæ—¢å­˜ï¼‰
# ---------------------------------------------------------------------------
def detect_equipment_from_question(question: str, available_equipment: List[str]) -> Optional[str]:
    """
    è³ªå•æ–‡ã‹ã‚‰å¯¾è±¡è¨­å‚™ã‚’æ¨å®š
    
    Args:
        question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•æ–‡
        available_equipment: åˆ©ç”¨å¯èƒ½ãªè¨­å‚™åã®ãƒªã‚¹ãƒˆ
        
    Returns:
        æ¨å®šã•ã‚ŒãŸè¨­å‚™åã¾ãŸã¯ None
    """
    # è³ªå•æ–‡ã‚’æ­£è¦åŒ–
    question_lower = question.lower()
    
    # è¨­å‚™åãŒç›´æ¥å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    for equipment in available_equipment:
        equipment_keywords = equipment.replace("è¨­å‚™", "").split("ãƒ»")
        
        for keyword in equipment_keywords:
            if keyword in question_lower:
                print(f"ğŸ¯ è‡ªå‹•æ¨å®š: '{keyword}' â†’ {equipment}")
                return equipment
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
    equipment_keywords = {
        "è‡ªå‹•ç«ç½å ±çŸ¥è¨­å‚™": ["ç«ç½", "æ„ŸçŸ¥å™¨", "ç…™", "ç†±", "å ±çŸ¥", "è­¦å ±"],
        "éå¸¸æ”¾é€è¨­å‚™": ["æ”¾é€", "ã‚¹ãƒ”ãƒ¼ã‚«", "ã‚¢ãƒŠã‚¦ãƒ³ã‚¹"],
        "èª˜å°ç¯è¨­å‚™": ["èª˜å°ç¯", "é¿é›£", "èª˜å°"],
        "éå¸¸ç…§æ˜è¨­å‚™": ["éå¸¸ç…§æ˜", "éå¸¸ç¯", "ç…§æ˜"]
    }
    
    for equipment, keywords in equipment_keywords.items():
        if equipment in available_equipment:
            for keyword in keywords:
                if keyword in question_lower:
                    print(f"ğŸ¯ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¨å®š: '{keyword}' â†’ {equipment}")
                    return equipment
    
    print("â“ è¨­å‚™ã‚’è‡ªå‹•æ¨å®šã§ãã¾ã›ã‚“ã§ã—ãŸ")
    return None

# ---------------------------------------------------------------------------
# äº’æ›æ€§ç¶­æŒï¼ˆæ—§é–¢æ•°ï¼‰
# ---------------------------------------------------------------------------
def generate_answer(*args, **kwargs):
    """æ—§é–¢æ•°ã®äº’æ›æ€§ç¶­æŒ - å»ƒæ­¢äºˆå®š"""
    raise NotImplementedError(
        "generate_answer ã¯å»ƒæ­¢ã•ã‚Œã¾ã—ãŸã€‚generate_answer_with_equipment ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
    )