# src/langchain_chains.py

from typing import List, Dict, Any, Optional
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

from src.langchain_models import get_chat_model
from src.logging_utils import init_logger
logger = init_logger()

class ChainManager:
    """LangChainç”¨ã®ãƒã‚§ãƒ¼ãƒ³ç®¡ç†ã‚¯ãƒ©ã‚¹ - ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆ"""
    
    @staticmethod
    def create_equipment_knowledge(inputs: dict) -> str:
        """è¨­å‚™è³‡æ–™ã®Knowledge Contentsç”Ÿæˆ"""
        equipment_content = inputs.get("equipment_content", "")
        if not equipment_content:
            return "è¨­å‚™è³‡æ–™æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
        return equipment_content
    
    @staticmethod
    def create_building_knowledge(inputs: dict) -> str:
        """ãƒ“ãƒ«æƒ…å ±ã®Knowledge Contentsç”Ÿæˆ"""
        building_content = inputs.get("building_content", "")
        if not building_content:
            return "ãƒ“ãƒ«æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
        return building_content
    
    @staticmethod
    def create_chat_history_messages(chat_history: Optional[List[Dict[str, str]]]) -> List:
        """ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’LangChainã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã«å¤‰æ›"""
        if not chat_history:
            return []
        
        messages = []
        for msg in chat_history:
            if not isinstance(msg, dict) or not msg.get("role") or not msg.get("content"):
                continue
                
            if msg["role"] == "user":
                messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                messages.append(AIMessage(content=msg["content"]))
            elif msg["role"] == "system":
                messages.append(SystemMessage(content=msg["content"]))
        
        return messages
    
    @staticmethod
    def create_unified_chain(
        model_name: str,
        system_prompt: str,
        mode: str,
        temperature: float = 0.0,
        max_tokens: Optional[int] = None
    ):
        """çµ±ä¸€ã•ã‚ŒãŸãƒã‚§ãƒ¼ãƒ³ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹é€ :
        === System Message ===
        ï¼ˆå„ãƒ¢ãƒ¼ãƒ‰å°‚ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
        === Knowledge Contents ===
        ï¼ˆå„ãƒ¢ãƒ¼ãƒ‰ã®è¿½åŠ è³‡æ–™æƒ…å ±ï¼‰
        === Chat History ===
        ï¼ˆã‚ã‚‹å ´åˆã¯ä¼šè©±å±¥æ­´ï¼‰
        === Human Message ===
        ã€è³ªå•ã€‘ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ï¼‰
        ä¸Šè¨˜ã®æƒ…å ±ã‚’å‚è€ƒã«ã€æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
        """
        chat_model = get_chat_model(model_name, temperature, max_tokens)
        
        if mode == "æš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰":
            # è¨­å‚™è³‡æ–™ã‚ã‚Šã®å ´åˆ
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("human", "=== Knowledge Contents ===\n{knowledge_contents}"),
                MessagesPlaceholder(variable_name="chat_history", optional=True),
                ("human", "ã€è³ªå•ã€‘\n{question}\n\nä¸Šè¨˜ã®æƒ…å ±ã‚’å‚è€ƒã«ã€æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚")
            ])
            
            knowledge_generator = RunnableLambda(ChainManager.create_equipment_knowledge)
            
        elif mode == "ãƒ“ãƒ«ãƒã‚¹ã‚¿è³ªå•ãƒ¢ãƒ¼ãƒ‰":
            # ãƒ“ãƒ«æƒ…å ±ã‚ã‚Šã®å ´åˆ
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("human", "=== Knowledge Contents ===\n{knowledge_contents}"),
                MessagesPlaceholder(variable_name="chat_history", optional=True),
                ("human", "ã€è³ªå•ã€‘\n{question}\n\nä¸Šè¨˜ã®æƒ…å ±ã‚’å‚è€ƒã«ã€æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚")
            ])
            
            knowledge_generator = RunnableLambda(ChainManager.create_building_knowledge)
            
        else:  # è³ªç–‘å¿œç­”æ›¸æ·»å‰Šãƒ¢ãƒ¼ãƒ‰
            # Knowledge Contentsãªã—ã®å ´åˆ
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                MessagesPlaceholder(variable_name="chat_history", optional=True),
                ("human", "ã€è³ªå•ã€‘\n{question}\n\nä¸Šè¨˜ã®æƒ…å ±ã‚’å‚è€ƒã«ã€æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚")
            ])
            
            knowledge_generator = None
        
        # ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
        if knowledge_generator:
            chain = (
                {
                    "question": lambda x: x["question"],
                    "knowledge_contents": knowledge_generator,
                    "chat_history": lambda x: ChainManager.create_chat_history_messages(x.get("chat_history"))
                }
                | prompt
                | chat_model
                | StrOutputParser()
            )
        else:
            chain = (
                {
                    "question": lambda x: x["question"],
                    "chat_history": lambda x: ChainManager.create_chat_history_messages(x.get("chat_history"))
                }
                | prompt
                | chat_model
                | StrOutputParser()
            )
        
        logger.info(f"âœ… Unified Chain ä½œæˆå®Œäº†: model={model_name}, mode={mode}")
        return chain

# === çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ ===

def generate_unified_answer(
    *,
    prompt: str,
    question: str,
    model: str = "claude-4-sonnet",
    mode: str = "æš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰",
    equipment_content: Optional[str] = None,
    building_content: Optional[str] = None,
    chat_history: Optional[List[Dict[str, str]]] = None,
    temperature: float = 0.0,
    max_tokens: Optional[int] = None
) -> Dict[str, Any]:
    """
    çµ±ä¸€ã•ã‚ŒãŸå›ç­”ç”Ÿæˆé–¢æ•°
    
    Args:
        prompt: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        mode: ãƒ¢ãƒ¼ãƒ‰å
        equipment_content: è¨­å‚™è³‡æ–™å†…å®¹ï¼ˆæš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã§ä½¿ç”¨ï¼‰
        building_content: ãƒ“ãƒ«æƒ…å ±å†…å®¹ï¼ˆãƒ“ãƒ«ãƒã‚¹ã‚¿è³ªå•ãƒ¢ãƒ¼ãƒ‰ã§ä½¿ç”¨ï¼‰
        chat_history: ãƒãƒ£ãƒƒãƒˆå±¥æ­´
        temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        
    Returns:
        å›ç­”çµæœè¾æ›¸
    """
    
    logger.info(f"ğŸš€ çµ±ä¸€å›ç­”ç”Ÿæˆé–‹å§‹: model={model}, mode={mode}")
    
    # çµ±ä¸€ãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ
    chain = ChainManager.create_unified_chain(model, prompt, mode, temperature, max_tokens)
    
    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿æº–å‚™
    chain_input = {
        "question": question,
        "chat_history": chat_history[:-1] if chat_history and len(chat_history) > 1 else None
    }
    
    # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¿½åŠ 
    if mode == "æš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰" and equipment_content:
        chain_input["equipment_content"] = equipment_content
    elif mode == "ãƒ“ãƒ«ãƒã‚¹ã‚¿è³ªå•ãƒ¢ãƒ¼ãƒ‰" and building_content:
        chain_input["building_content"] = building_content
    
    # ãƒã‚§ãƒ¼ãƒ³å®Ÿè¡Œ
    try:
        answer = chain.invoke(chain_input)
        
        # çµæœæ§‹ç¯‰
        result = {
            "answer": answer,
            "mode": mode,
            "langchain_used": True
        }
        
        logger.info(f"âœ… çµ±ä¸€å›ç­”ç”Ÿæˆå®Œäº†: mode={mode}, å›ç­”æ–‡å­—æ•°={len(answer)}")
        return result
        
    except Exception as e:
        logger.error(f"âŒ çµ±ä¸€å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        raise

# === å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®é–¢æ•° ===

def generate_smart_answer_with_langchain(
    *,
    prompt: str,
    question: str,
    model: str = "claude-4-sonnet",
    equipment_data: Optional[Dict[str, Dict[str, Any]]] = None,
    chat_history: Optional[List[Dict[str, str]]] = None,
    temperature: float = 0.0,
    max_tokens: Optional[int] = None
) -> Dict[str, Any]:
    """
    æ—¢å­˜ã®app.pyã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹é–¢æ•°ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
    
    æ³¨æ„: ã“ã®é–¢æ•°ã¯æ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒã—ã¤ã¤ã€
    å®Ÿéš›ã®è¨­å‚™é¸æŠã‚„ãƒ“ãƒ«æƒ…å ±å–å¾—ã¯app.pyå´ã§è¡Œã‚ã‚Œã‚‹å‰æã§ã™ã€‚
    """
    
    # ã“ã®é–¢æ•°ã¯æ—¢å­˜ã®app.pyã¨ã®äº’æ›æ€§ã‚’ä¿ã¤ãŸã‚ã€
    # å®Ÿéš›ã®å‡¦ç†ã¯app.pyå´ã§è¡Œã†ã“ã¨ã‚’æƒ³å®š
    # ã“ã“ã§ã¯åŸºæœ¬çš„ãªå›ç­”ç”Ÿæˆã®ã¿å®Ÿè¡Œ
    
    return generate_unified_answer(
        prompt=prompt,
        question=question,
        model=model,
        mode="æš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰",  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        chat_history=chat_history,
        temperature=temperature,
        max_tokens=max_tokens
    )

# ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°
def test_chain_creation():
    """ãƒã‚§ãƒ¼ãƒ³ä½œæˆã®ãƒ†ã‚¹ãƒˆ"""
    try:
        logger.info("ğŸ§ª çµ±ä¸€ãƒã‚§ãƒ¼ãƒ³ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        # å„ãƒ¢ãƒ¼ãƒ‰ã®ãƒ†ã‚¹ãƒˆ
        modes = ["æš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰", "è³ªç–‘å¿œç­”æ›¸æ·»å‰Šãƒ¢ãƒ¼ãƒ‰", "ãƒ“ãƒ«ãƒã‚¹ã‚¿è³ªå•ãƒ¢ãƒ¼ãƒ‰"]
        
        for mode in modes:
            chain = ChainManager.create_unified_chain(
                "claude-4-sonnet",
                f"ã‚ãªãŸã¯{mode}ã®å°‚é–€å®¶ã§ã™ã€‚",
                mode,
                temperature=0.0
            )
            logger.info(f"âœ… {mode} Chain ä½œæˆæˆåŠŸ")
        
        logger.info("ğŸ§ª çµ±ä¸€å›ç­”ç”Ÿæˆãƒ†ã‚¹ãƒˆ...")
        result = generate_unified_answer(
            prompt="ãƒ†ã‚¹ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            question="ãƒ†ã‚¹ãƒˆè³ªå•",
            mode="è³ªç–‘å¿œç­”æ›¸æ·»å‰Šãƒ¢ãƒ¼ãƒ‰"
        )
        logger.info("âœ… çµ±ä¸€å›ç­”ç”Ÿæˆãƒ†ã‚¹ãƒˆæˆåŠŸ")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    test_chain_creation()