# src/langchain_chains.py

from typing import List, Dict, Any, Optional
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

from src.langchain_models import get_chat_model
from src.logging_utils import init_logger
logger = init_logger()

class ChainManager:
    """LangChainç”¨ã®ãƒã‚§ãƒ¼ãƒ³ç®¡ç†ã‚¯ãƒ©ã‚¹ - ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆ"""
    @staticmethod
    def create_combined_knowledge(inputs: dict) -> str:
        """è¨­å‚™è³‡æ–™ã¨ãƒ“ãƒ«æƒ…å ±ã‚’çµ„ã¿åˆã‚ã›ãŸKnowledge Contentsç”Ÿæˆ"""
        equipment_content = inputs.get("equipment_content", "")
        building_content = inputs.get("building_content", "")
        
        knowledge_parts = []
        
        if equipment_content:
            knowledge_parts.append(f"=== è¨­å‚™è³‡æ–™æƒ…å ± ===\n{equipment_content}")
        
        if building_content:
            knowledge_parts.append(f"=== ãƒ“ãƒ«æƒ…å ± ===\n{building_content}")
        
        if not knowledge_parts:
            return "é–¢é€£è³‡æ–™æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ä¸€èˆ¬çŸ¥è­˜ã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚"
        
        return "\n\n".join(knowledge_parts)
    
    @staticmethod
    def create_chat_history_messages(chat_history: Optional[List[Dict[str, str]]]) -> List:
        """ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’LangChainã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã«å¤‰æ›"""
        if not chat_history:
            return []
        
        messages = []
        for msg in chat_history:
            if not isinstance(msg, dict) or not msg.get("role") or not msg.get("content"):
                continue
                
            if msg["role"] == "user":
                messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                messages.append(AIMessage(content=msg["content"]))
            elif msg["role"] == "system":
                messages.append(SystemMessage(content=msg["content"]))
        
        return messages
    
    @staticmethod
    def create_unified_chain(
        model_name: str,
        system_prompt: str,
        mode: str,
        temperature: float = 0.0,
        max_tokens: Optional[int] = None
    ):
        """çµ±ä¸€ã•ã‚ŒãŸãƒã‚§ãƒ¼ãƒ³ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ - ãƒ¢ãƒ¼ãƒ‰åˆ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹æˆå¯¾å¿œ"""
        chat_model = get_chat_model(model_name, temperature, max_tokens)
        
        if mode == "è³ªç–‘å¿œç­”æ›¸æ·»å‰Šãƒ¢ãƒ¼ãƒ‰":
            # Knowledge Contentsãªã—ã®å ´åˆ
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                MessagesPlaceholder(variable_name="chat_history", optional=True),
                ("human", "ã€æ·»å‰Šä¾é ¼ã€‘\n{question}\n\nä¸Šè¨˜ã®å†…å®¹ã«ã¤ã„ã¦ã€è³ªç–‘å¿œç­”æ›¸ã¨ã—ã¦é©åˆ‡ãªå½¢å¼ã§æ·»å‰Šãƒ»æ”¹å–„ææ¡ˆã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚")
            ])
            knowledge_generator = None
            
        elif mode == "æš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰":
            # æš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰å°‚ç”¨æ§‹æˆ
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("human", "=== è¨­å‚™è³‡æ–™æƒ…å ± ===\n{equipment_content}\n\n=== ãƒ“ãƒ«æƒ…å ± ===\n{building_content}"),
                MessagesPlaceholder(variable_name="chat_history", optional=True),
                ("human", "ã€æŠ€è¡“çš„è³ªå•ã€‘\n{question}\n\nä¸Šè¨˜ã®è¨­å‚™è³‡æ–™ã¨ãƒ“ãƒ«æƒ…å ±ã‚’å‚è€ƒã«ã€å»ºç¯‰é›»æ°—è¨­å‚™è¨­è¨ˆã®è¦³ç‚¹ã‹ã‚‰è©³ç´°ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚")
            ])
            knowledge_generator = RunnableLambda(ChainManager.create_separate_knowledge)
            
        elif mode == "ãƒ“ãƒ«ãƒã‚¹ã‚¿è³ªå•ãƒ¢ãƒ¼ãƒ‰":
            # ãƒ“ãƒ«ãƒã‚¹ã‚¿è³ªå•ãƒ¢ãƒ¼ãƒ‰å°‚ç”¨æ§‹æˆ
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("human", "=== ãƒ“ãƒ«ãƒã‚¹ã‚¿ãƒ¼æƒ…å ± ===\n{building_content}"),
                MessagesPlaceholder(variable_name="chat_history", optional=True),
                ("human", "ã€ãƒ“ãƒ«æƒ…å ±ã«é–¢ã™ã‚‹è³ªå•ã€‘\n{question}\n\nãƒ“ãƒ«ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹æƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ã€æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚")
            ])
            knowledge_generator = RunnableLambda(ChainManager.create_building_knowledge)
            
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆæ—¢å­˜ã®çµ±ä¸€æ§‹æˆã‚’ç¶­æŒï¼‰
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("human", "{knowledge_contents}"),
                MessagesPlaceholder(variable_name="chat_history", optional=True),
                ("human", "ã€è³ªå•ã€‘\n{question}\n\nä¸Šè¨˜ã®è³‡æ–™æƒ…å ±ã‚’å‚è€ƒã«ã€æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚")
            ])
            knowledge_generator = RunnableLambda(ChainManager.create_combined_knowledge)
        
        # ğŸ”¥ ä¿®æ­£: ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰ã‚’çµ±ä¸€ï¼ˆãƒ¢ãƒ¼ãƒ‰åˆ¥ã§ã‚‚åŒã˜æ§‹é€ ï¼‰
        if knowledge_generator:
            chain = (
                {
                    "question": lambda x: x["question"],
                    "equipment_content": lambda x: x.get("equipment_content", ""),
                    "building_content": lambda x: x.get("building_content", ""),
                    "knowledge_contents": knowledge_generator,  # å¾“æ¥ãƒ¢ãƒ¼ãƒ‰ç”¨
                    "chat_history": lambda x: ChainManager.create_chat_history_messages(x.get("chat_history"))
                }
                | prompt
                | chat_model
                | StrOutputParser()
            )
        else:
            chain = (
                {
                    "question": lambda x: x["question"],
                    "chat_history": lambda x: ChainManager.create_chat_history_messages(x.get("chat_history"))
                }
                | prompt
                | chat_model
                | StrOutputParser()
            )
        
        logger.info(f"âœ… Unified Chain ä½œæˆå®Œäº†: model={model_name}, mode={mode}")
        return chain

# æ–°ã—ã„ãƒŠãƒ¬ãƒƒã‚¸ç”Ÿæˆé–¢æ•°ã‚’2ã¤ã ã‘è¿½åŠ 

@staticmethod
def create_separate_knowledge(inputs: dict) -> dict:
    """æš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ç”¨ï¼šè¨­å‚™ã¨ãƒ“ãƒ«ã‚’åˆ†é›¢ã—ã¦è¿”ã™"""
    result = inputs.copy()
    result["equipment_content"] = inputs.get("equipment_content", "è¨­å‚™è³‡æ–™æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    result["building_content"] = inputs.get("building_content", "ãƒ“ãƒ«æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    return result

@staticmethod
def create_building_knowledge(inputs: dict) -> dict:
    """ãƒ“ãƒ«ãƒã‚¹ã‚¿è³ªå•ãƒ¢ãƒ¼ãƒ‰ç”¨ï¼šãƒ“ãƒ«æƒ…å ±ã®ã¿è¿”ã™"""
    result = inputs.copy()
    result["building_content"] = inputs.get("building_content", "ãƒ“ãƒ«æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    return result

# === çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ ===

def generate_unified_answer(
    *,
    prompt: str,
    question: str,
    model: str = "claude-4-sonnet",
    mode: str = "æš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰",
    equipment_content: Optional[str] = None,
    building_content: Optional[str] = None,
    chat_history: Optional[List[Dict[str, str]]] = None,
    temperature: float = 0.0,
    max_tokens: Optional[int] = None
) -> Dict[str, Any]:
    """
    çµ±ä¸€ã•ã‚ŒãŸå›ç­”ç”Ÿæˆé–¢æ•°
    """
    
    logger.info(f"ğŸš€ çµ±ä¸€å›ç­”ç”Ÿæˆé–‹å§‹: model={model}, mode={mode}")
    
    # çµ±ä¸€ãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ
    chain = ChainManager.create_unified_chain(model, prompt, mode, temperature, max_tokens)
    
    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿æº–å‚™
    chain_input = {
        "question": question,
        "chat_history": chat_history[:-1] if chat_history and len(chat_history) > 1 else None
    }
    
    # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¿½åŠ 
    if mode != "è³ªç–‘å¿œç­”æ›¸æ·»å‰Šãƒ¢ãƒ¼ãƒ‰":
        chain_input["equipment_content"] = equipment_content or ""
        chain_input["building_content"] = building_content or ""
    
    # ãƒã‚§ãƒ¼ãƒ³å®Ÿè¡Œ
    try:
        answer = chain.invoke(chain_input)
        
        # ğŸ”¥ ä¿®æ­£: ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®complete_promptæ§‹ç¯‰
        full_prompt_parts = []
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        full_prompt_parts.append(f"=== System Message ===\n{prompt}")
        
        # ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®Knowledge Contentsæ§‹ç¯‰
        if mode == "æš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰":
            # è¨­å‚™ã¨ãƒ“ãƒ«æƒ…å ±ã‚’åˆ†é›¢è¡¨ç¤º
            equipment_content = chain_input.get("equipment_content", "")
            building_content = chain_input.get("building_content", "")
            
            knowledge_parts = []
            if equipment_content:
                knowledge_parts.append(f"=== è¨­å‚™è³‡æ–™æƒ…å ± ===\n{equipment_content}")
            if building_content:
                knowledge_parts.append(f"=== ãƒ“ãƒ«æƒ…å ± ===\n{building_content}")
            
            if knowledge_parts:
                full_prompt_parts.append("\n\n".join(knowledge_parts))
            else:
                full_prompt_parts.append("=== Knowledge Contents ===\nè¨­å‚™è³‡æ–™æƒ…å ±ãŠã‚ˆã³ãƒ“ãƒ«æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                
        elif mode == "ãƒ“ãƒ«ãƒã‚¹ã‚¿è³ªå•ãƒ¢ãƒ¼ãƒ‰":
            # ãƒ“ãƒ«æƒ…å ±ã®ã¿
            building_content = chain_input.get("building_content", "")
            if building_content:
                full_prompt_parts.append(f"=== ãƒ“ãƒ«ãƒã‚¹ã‚¿ãƒ¼æƒ…å ± ===\n{building_content}")
            else:
                full_prompt_parts.append("=== ãƒ“ãƒ«ãƒã‚¹ã‚¿ãƒ¼æƒ…å ± ===\nãƒ“ãƒ«æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
                
        elif mode != "è³ªç–‘å¿œç­”æ›¸æ·»å‰Šãƒ¢ãƒ¼ãƒ‰":
            # ãã®ä»–ã®ãƒ¢ãƒ¼ãƒ‰ï¼ˆå¾“æ¥ã®çµ±ä¸€æ§‹é€ ï¼‰
            equipment_content = chain_input.get("equipment_content", "")
            building_content = chain_input.get("building_content", "")
            
            knowledge_parts = []
            if equipment_content:
                knowledge_parts.append(f"=== è¨­å‚™è³‡æ–™æƒ…å ± ===\n{equipment_content}")
            if building_content:
                knowledge_parts.append(f"=== ãƒ“ãƒ«æƒ…å ± ===\n{building_content}")
            
            if knowledge_parts:
                full_prompt_parts.append(f"=== Knowledge Contents ===\n" + "\n\n".join(knowledge_parts))
            else:
                full_prompt_parts.append("=== Knowledge Contents ===\né–¢é€£è³‡æ–™æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ï¼ˆå…ƒã®è¾æ›¸å½¢å¼ã‹ã‚‰ç›´æ¥å–å¾—ï¼‰
        original_chat_history = chat_history[:-1] if chat_history and len(chat_history) > 1 else None
        if original_chat_history:
            full_prompt_parts.append("=== Chat History ===")
            for msg in original_chat_history:
                if isinstance(msg, dict) and msg.get("role") and msg.get("content"):
                    role = msg["role"].capitalize()
                    full_prompt_parts.append(f"{role}: {msg['content']}")
        
        # ç¾åœ¨ã®è³ªå•ï¼ˆãƒ¢ãƒ¼ãƒ‰åˆ¥ã®æ¥é ­è¾ä»˜ãï¼‰
        if mode == "æš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰":
            full_prompt_parts.append(f"=== Human Message ===\nã€æŠ€è¡“çš„è³ªå•ã€‘\n{question}\n\nä¸Šè¨˜ã®è¨­å‚™è³‡æ–™ã¨ãƒ“ãƒ«æƒ…å ±ã‚’å‚è€ƒã«ã€å»ºç¯‰é›»æ°—è¨­å‚™è¨­è¨ˆã®è¦³ç‚¹ã‹ã‚‰è©³ç´°ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚")
        elif mode == "è³ªç–‘å¿œç­”æ›¸æ·»å‰Šãƒ¢ãƒ¼ãƒ‰":
            full_prompt_parts.append(f"=== Human Message ===\nã€æ·»å‰Šä¾é ¼ã€‘\n{question}\n\nä¸Šè¨˜ã®å†…å®¹ã«ã¤ã„ã¦ã€è³ªç–‘å¿œç­”æ›¸ã¨ã—ã¦é©åˆ‡ãªå½¢å¼ã§æ·»å‰Šãƒ»æ”¹å–„ææ¡ˆã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚")
        elif mode == "ãƒ“ãƒ«ãƒã‚¹ã‚¿è³ªå•ãƒ¢ãƒ¼ãƒ‰":
            full_prompt_parts.append(f"=== Human Message ===\nã€ãƒ“ãƒ«æƒ…å ±ã«é–¢ã™ã‚‹è³ªå•ã€‘\n{question}\n\nãƒ“ãƒ«ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹æƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ã€æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚")
        else:
            full_prompt_parts.append(f"=== Human Message ===\nã€è³ªå•ã€‘\n{question}\n\nä¸Šè¨˜ã®è³‡æ–™æƒ…å ±ã‚’å‚è€ƒã«ã€æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚")
        
        # å®Œå…¨ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµåˆ
        complete_prompt = "\n\n".join(full_prompt_parts)
        
        # çµæœæ§‹ç¯‰
        result = {
            "answer": answer,
            "mode": mode,
            "langchain_used": True,
            "complete_prompt": complete_prompt  # ğŸ”¥ ãƒ¢ãƒ¼ãƒ‰åˆ¥æ§‹é€ ã«å¯¾å¿œ
        }
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ çµ±ä¸€å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        raise

# === å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®é–¢æ•° ===

def generate_smart_answer_with_langchain(
    *,
    prompt: str,
    question: str,
    model: str = "claude-4-sonnet",
    mode: str = "æš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰",
    equipment_content: Optional[str] = None,
    building_content: Optional[str] = None,
    chat_history: Optional[List[Dict[str, str]]] = None,
    temperature: float = 0.0,
    max_tokens: Optional[int] = None
) -> Dict[str, Any]:
    """
    æ—¢å­˜ã®app.pyã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹é–¢æ•°ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
    
    æ³¨æ„: ã“ã®é–¢æ•°ã¯æ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒã—ã¤ã¤ã€
    å®Ÿéš›ã®è¨­å‚™é¸æŠã‚„ãƒ“ãƒ«æƒ…å ±å–å¾—ã¯app.pyå´ã§è¡Œã‚ã‚Œã‚‹å‰æã§ã™ã€‚
    """
    
    # ã“ã®é–¢æ•°ã¯æ—¢å­˜ã®app.pyã¨ã®äº’æ›æ€§ã‚’ä¿ã¤ãŸã‚ã€
    # å®Ÿéš›ã®å‡¦ç†ã¯app.pyå´ã§è¡Œã†ã“ã¨ã‚’æƒ³å®š
    # ã“ã“ã§ã¯åŸºæœ¬çš„ãªå›ç­”ç”Ÿæˆã®ã¿å®Ÿè¡Œ
    
    return generate_unified_answer(
        prompt=prompt,
        question=question,
        model=model,
        mode=mode,
        equipment_content=equipment_content,
        building_content=building_content,
        chat_history=chat_history,
        temperature=temperature,
        max_tokens=max_tokens
    )

# ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°
def test_chain_creation():
    """ãƒã‚§ãƒ¼ãƒ³ä½œæˆã®ãƒ†ã‚¹ãƒˆ"""
    try:
        logger.info("ğŸ§ª çµ±ä¸€ãƒã‚§ãƒ¼ãƒ³ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        # å„ãƒ¢ãƒ¼ãƒ‰ã®ãƒ†ã‚¹ãƒˆ
        modes = ["æš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰", "è³ªç–‘å¿œç­”æ›¸æ·»å‰Šãƒ¢ãƒ¼ãƒ‰", "ãƒ“ãƒ«ãƒã‚¹ã‚¿è³ªå•ãƒ¢ãƒ¼ãƒ‰"]
        
        for mode in modes:
            chain = ChainManager.create_unified_chain(
                "claude-4-sonnet",
                f"ã‚ãªãŸã¯{mode}ã®å°‚é–€å®¶ã§ã™ã€‚",
                mode,
                temperature=0.0
            )
            logger.info(f"âœ… {mode} Chain ä½œæˆæˆåŠŸ")
        
        logger.info("ğŸ§ª çµ±ä¸€å›ç­”ç”Ÿæˆãƒ†ã‚¹ãƒˆ...")
        result = generate_unified_answer(
            prompt="ãƒ†ã‚¹ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            question="ãƒ†ã‚¹ãƒˆè³ªå•",
            mode="è³ªç–‘å¿œç­”æ›¸æ·»å‰Šãƒ¢ãƒ¼ãƒ‰"
        )
        logger.info("âœ… çµ±ä¸€å›ç­”ç”Ÿæˆãƒ†ã‚¹ãƒˆæˆåŠŸ")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    test_chain_creation()