# src/langchain_chains.py (æœ€å°é™ã®å¤‰æ›´ã‚’åŠ ãˆãŸæœ€çµ‚ç‰ˆ)

from typing import List, Dict, Any, Optional
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableLambda
# â–¼ å¤‰æ›´ç‚¹ï¼šJSONãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

from src.langchain_models import get_chat_model
from src.logging_utils import init_logger
logger = init_logger()

class ChainManager:
    """LangChainç”¨ã®ãƒã‚§ãƒ¼ãƒ³ç®¡ç†ã‚¯ãƒ©ã‚¹ - ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆ"""
    # =================================================================
    # â–¼ å¤‰æ›´ç‚¹
    # ã“ã®ã‚¯ãƒ©ã‚¹å†…ã®æ—¢å­˜ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯ä¸€åˆ‡å¤‰æ›´ã—ã¾ã›ã‚“ã€‚
    # =================================================================
    @staticmethod
    def create_combined_knowledge(inputs: dict) -> str:
        """è¨­å‚™è³‡æ–™ã¨ãƒ“ãƒ«æƒ…å ±ã‚’çµ„ã¿åˆã‚ã›ãŸKnowledge Contentsç”Ÿæˆ"""
        equipment_content = inputs.get("equipment_content", "")
        building_content = inputs.get("building_content", "")
        
        knowledge_parts = []
        
        if equipment_content:
            knowledge_parts.append(f"=== è¨­å‚™è³‡æ–™æƒ…å ± ===\n{equipment_content}")
        
        if building_content:
            knowledge_parts.append(f"=== ãƒ“ãƒ«æƒ…å ± ===\n{building_content}")
        
        if not knowledge_parts:
            return "é–¢é€£è³‡æ–™æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ä¸€èˆ¬çŸ¥è­˜ã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚"
        
        return "\n\n".join(knowledge_parts)
    
    @staticmethod
    def create_chat_history_messages(chat_history: Optional[List[Dict[str, str]]]) -> List:
        """ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’LangChainã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã«å¤‰æ›"""
        if not chat_history:
            return []
        
        messages = []
        for msg in chat_history:
            if not isinstance(msg, dict) or not msg.get("role") or not msg.get("content"):
                continue
                
            if msg["role"] == "user":
                messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                messages.append(AIMessage(content=msg["content"]))
            elif msg["role"] == "system":
                messages.append(SystemMessage(content=msg["content"]))
        
        return messages
    
    @staticmethod
    def create_unified_chain(
        model_name: str,
        system_prompt: str,
        mode: str,
        temperature: float = 0.0,
        max_tokens: Optional[int] = None
    ):
        """çµ±ä¸€ã•ã‚ŒãŸãƒã‚§ãƒ¼ãƒ³ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ - ãƒ¢ãƒ¼ãƒ‰åˆ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹æˆå¯¾å¿œ"""
        chat_model = get_chat_model(model_name, temperature, max_tokens)
        
        if mode == "è³ªç–‘å¿œç­”æ›¸æ·»å‰Šãƒ¢ãƒ¼ãƒ‰":
            # Knowledge Contentsãªã—ã®å ´åˆ
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                MessagesPlaceholder(variable_name="chat_history", optional=True),
                ("human", "ã€æ·»å‰Šä¾é ¼ã€‘\n{question}\n\nä¸Šè¨˜ã®å†…å®¹ã«ã¤ã„ã¦ã€è³ªç–‘å¿œç­”æ›¸ã¨ã—ã¦é©åˆ‡ãªå½¢å¼ã§æ·»å‰Šãƒ»æ”¹å–„ææ¡ˆã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚")
            ])
            knowledge_generator = None
            
        elif mode == "æš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰":
            # æš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰å°‚ç”¨æ§‹æˆ
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("human", "=== è¨­å‚™è³‡æ–™æƒ…å ± ===\n{equipment_content}\n\n=== ãƒ“ãƒ«æƒ…å ± ===\n{building_content}"),
                MessagesPlaceholder(variable_name="chat_history", optional=True),
                ("human", "ã€æŠ€è¡“çš„è³ªå•ã€‘\n{question}\n\nä¸Šè¨˜ã®è¨­å‚™è³‡æ–™ã¨ãƒ“ãƒ«æƒ…å ±ã‚’å‚è€ƒã«ã€å»ºç¯‰é›»æ°—è¨­å‚™è¨­è¨ˆã®è¦³ç‚¹ã‹ã‚‰è©³ç´°ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚")
            ])
            knowledge_generator = RunnableLambda(ChainManager.create_separate_knowledge)
            
        elif mode == "ãƒ“ãƒ«ãƒã‚¹ã‚¿è³ªå•ãƒ¢ãƒ¼ãƒ‰":
            # ãƒ“ãƒ«ãƒã‚¹ã‚¿è³ªå•ãƒ¢ãƒ¼ãƒ‰å°‚ç”¨æ§‹æˆ
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("human", "=== ãƒ“ãƒ«ãƒã‚¹ã‚¿ãƒ¼æƒ…å ± ===\n{building_content}"),
                MessagesPlaceholder(variable_name="chat_history", optional=True),
                ("human", "ã€ãƒ“ãƒ«æƒ…å ±ã«é–¢ã™ã‚‹è³ªå•ã€‘\n{question}\n\nãƒ“ãƒ«ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹æƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ã€æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚")
            ])
            knowledge_generator = RunnableLambda(ChainManager.create_building_knowledge)
            
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆæ—¢å­˜ã®çµ±ä¸€æ§‹æˆã‚’ç¶­æŒï¼‰
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("human", "{knowledge_contents}"),
                MessagesPlaceholder(variable_name="chat_history", optional=True),
                ("human", "ã€è³ªå•ã€‘\n{question}\n\nä¸Šè¨˜ã®è³‡æ–™æƒ…å ±ã‚’å‚è€ƒã«ã€æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚")
            ])
            knowledge_generator = RunnableLambda(ChainManager.create_combined_knowledge)
        
        # ğŸ”¥ ä¿®æ­£: ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰ã‚’çµ±ä¸€ï¼ˆæ–°ã—ã„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å¯¾å¿œï¼‰
        if knowledge_generator:
            chain = (
                {
                    "question": lambda x: x["question"],
                    "equipment_content": lambda x: x.get("equipment_content", ""),
                    "building_content": lambda x: x.get("building_content", ""),
                    "target_building_content": lambda x: x.get("target_building_content", ""),
                    "other_buildings_content": lambda x: x.get("other_buildings_content", ""),
                    "knowledge_contents": knowledge_generator,
                    "chat_history": lambda x: ChainManager.create_chat_history_messages(x.get("chat_history"))
                }
                | prompt
                | chat_model
                | StrOutputParser()
            )
        else:
            chain = (
                {
                    "question": lambda x: x["question"],
                    "chat_history": lambda x: ChainManager.create_chat_history_messages(x.get("chat_history"))
                }
                | prompt
                | chat_model
                | StrOutputParser()
            )
        
        logger.info(f"âœ… Unified Chain ä½œæˆå®Œäº†: model={model_name}, mode={mode}")
        return chain

    @staticmethod
    def create_building_knowledge(inputs: dict) -> dict:
        """ãƒ“ãƒ«ãƒã‚¹ã‚¿è³ªå•ãƒ¢ãƒ¼ãƒ‰ç”¨ï¼šæ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹é€ å¯¾å¿œ"""
        result = inputs.copy()
        
        target_building_content = inputs.get("target_building_content", "")
        other_buildings_content = inputs.get("other_buildings_content", "")
        building_content = inputs.get("building_content", "")
        
        if target_building_content and other_buildings_content:
            formatted_content = f"==ç¾åœ¨ã®å¯¾è±¡ãƒ“ãƒ«==\n{target_building_content}\n\n==ãã®ä»–ã®ãƒ“ãƒ«==\n{other_buildings_content}"
        elif target_building_content and not other_buildings_content:
            formatted_content = f"==ç¾åœ¨ã®å¯¾è±¡ãƒ“ãƒ«==\n{target_building_content}\n\n==ãã®ä»–ã®ãƒ“ãƒ«==\nãã®ä»–ã®ãƒ“ãƒ«æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
        elif other_buildings_content and not target_building_content:
            formatted_content = f"==ç¾åœ¨ã®å¯¾è±¡ãƒ“ãƒ«==\nå¯¾è±¡ãƒ“ãƒ«ã¯æŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n\n==ãã®ä»–ã®ãƒ“ãƒ«==\n{other_buildings_content}"
        elif building_content:
            formatted_content = f"==ç¾åœ¨ã®å¯¾è±¡ãƒ“ãƒ«==\nå¯¾è±¡ãƒ“ãƒ«ã¯æŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n\n==ãã®ä»–ã®ãƒ“ãƒ«==\n{building_content}"
        else:
            formatted_content = "==ç¾åœ¨ã®å¯¾è±¡ãƒ“ãƒ«==\nå¯¾è±¡ãƒ“ãƒ«ã¯æŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n\n==ãã®ä»–ã®ãƒ“ãƒ«==\nãƒ“ãƒ«æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
        
        result["building_content"] = formatted_content
        return result

    @staticmethod
    def create_separate_knowledge(inputs: dict) -> dict:
        """æš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ç”¨ï¼šè¨­å‚™ã¨ãƒ“ãƒ«ã‚’åˆ†é›¢è¡¨ç¤º"""
        result = inputs.copy()
        
        equipment_content = inputs.get("equipment_content", "")
        target_building_content = inputs.get("target_building_content", "")
        other_buildings_content = inputs.get("other_buildings_content", "")
        building_content = inputs.get("building_content", "")
        
        result["equipment_content"] = equipment_content if equipment_content else "è¨­å‚™è³‡æ–™æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
        
        if target_building_content and other_buildings_content:
            formatted_building = f"==ç¾åœ¨ã®å¯¾è±¡ãƒ“ãƒ«==\n{target_building_content}\n\n==ãã®ä»–ã®ãƒ“ãƒ«==\n{other_buildings_content}"
        elif target_building_content and not other_buildings_content:
            formatted_building = f"==ç¾åœ¨ã®å¯¾è±¡ãƒ“ãƒ«==\n{target_building_content}\n\n==ãã®ä»–ã®ãƒ“ãƒ«==\nãã®ä»–ã®ãƒ“ãƒ«æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
        elif building_content:
            formatted_building = building_content
        else:
            formatted_building = "ãƒ“ãƒ«æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
        
        result["building_content"] = formatted_building
        return result

    @staticmethod
    def create_building_prompt_content(inputs: dict) -> str:
        """complete_promptæ§‹ç¯‰ç”¨ï¼šãƒ“ãƒ«æƒ…å ±ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        target_building_content = inputs.get("target_building_content", "")
        other_buildings_content = inputs.get("other_buildings_content", "")
        building_content = inputs.get("building_content", "")
        
        if target_building_content and other_buildings_content:
            return f"=== ãƒ“ãƒ«ãƒã‚¹ã‚¿ãƒ¼æƒ…å ± ===\n==ç¾åœ¨ã®å¯¾è±¡ãƒ“ãƒ«==\n{target_building_content}\n\n==ãã®ä»–ã®ãƒ“ãƒ«==\n{other_buildings_content}"
        elif target_building_content and not other_buildings_content:
            return f"=== ãƒ“ãƒ«ãƒã‚¹ã‚¿ãƒ¼æƒ…å ± ===\n==ç¾åœ¨ã®å¯¾è±¡ãƒ“ãƒ«==\n{target_building_content}\n\n==ãã®ä»–ã®ãƒ“ãƒ«==\nãã®ä»–ã®ãƒ“ãƒ«æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
        elif other_buildings_content and not target_building_content:
            return f"=== ãƒ“ãƒ«ãƒã‚¹ã‚¿ãƒ¼æƒ…å ± ===\n==ç¾åœ¨ã®å¯¾è±¡ãƒ“ãƒ«==\nå¯¾è±¡ãƒ“ãƒ«ã¯æŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n\n==ãã®ä»–ã®ãƒ“ãƒ«==\n{other_buildings_content}"
        elif building_content:
            return f"=== ãƒ“ãƒ«ãƒã‚¹ã‚¿ãƒ¼æƒ…å ± ===\n{building_content}"
        else:
            return "=== ãƒ“ãƒ«ãƒã‚¹ã‚¿ãƒ¼æƒ…å ± ===\nãƒ“ãƒ«æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"

# =================================================================
# â–¼ å¤‰æ›´ç‚¹
# generate_unified_answer ã¨ generate_smart_answer_with_langchain ã‚’æ›¸ãæ›ãˆã€
# ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆæ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¾ã™ã€‚
# =================================================================

def get_actual_prompt_from_template(
    prompt_template: ChatPromptTemplate,
    inputs: dict,
    mode: str
) -> str:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‹ã‚‰å®Ÿéš›ã®é€ä¿¡å†…å®¹ã‚’å–å¾—"""
    try:
        # knowledge_generatorå‡¦ç†ãŒå¿…è¦ãªå ´åˆ
        if mode == "æš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰":
            processed_inputs = ChainManager.create_separate_knowledge(inputs)
        elif mode == "ãƒ“ãƒ«ãƒã‚¹ã‚¿è³ªå•ãƒ¢ãƒ¼ãƒ‰":
            processed_inputs = ChainManager.create_building_knowledge(inputs)
        elif mode != "è³ªç–‘å¿œç­”æ›¸æ·»å‰Šãƒ¢ãƒ¼ãƒ‰":
            processed_inputs = inputs.copy()
            processed_inputs["knowledge_contents"] = ChainManager.create_combined_knowledge(inputs)
        else:
            processed_inputs = inputs
        
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã«å¤‰æ›
        processed_inputs["chat_history"] = ChainManager.create_chat_history_messages(
            inputs.get("chat_history")
        )
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨
        messages = prompt_template.format_messages(**processed_inputs)
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        prompt_parts = []
        for msg in messages:
            role = getattr(msg, 'type', 'unknown').upper()
            content = getattr(msg, 'content', str(msg))
            prompt_parts.append(f"=== {role} ===\n{content}")
        
        complete_prompt = "\n\n" + ("="*50 + "\n\n").join(prompt_parts)
        
        logger.info(f"ğŸ”¥ Generated actual prompt: {len(complete_prompt)} characters")
        return complete_prompt
        
    except Exception as e:
        logger.error(f"âŒ Prompt generation failed: {e}")
        return f"=== ERROR ===\nãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã«å¤±æ•—: {str(e)}"

def generate_unified_answer(
    *,
    prompt: str,
    question: str,
    model: str = "claude-4-sonnet",
    mode: str = "æš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰",
    equipment_content: Optional[str] = None,
    building_content: Optional[str] = None,
    target_building_content: Optional[str] = None,
    other_buildings_content: Optional[str] = None,
    chat_history: Optional[List[Dict[str, str]]] = None,
    temperature: float = 0.0,
    max_tokens: Optional[int] = None,
    generate_title: bool = False # â˜…ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆãƒ•ãƒ©ã‚°ã‚’è¿½åŠ 
) -> Dict[str, Any]:
    """
    çµ±ä¸€ã•ã‚ŒãŸå›ç­”ç”Ÿæˆé–¢æ•°ã€‚generate_titleãƒ•ãƒ©ã‚°ã«å¿œã˜ã¦å‹•ä½œã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã€‚
    """
    logger.info(f"ğŸš€ çµ±ä¸€å›ç­”ç”Ÿæˆé–‹å§‹: model={model}, mode={mode}, generate_title={generate_title}")
    
    # æ—¢å­˜ã®ãƒã‚§ãƒ¼ãƒ³ä½œæˆãƒ­ã‚¸ãƒƒã‚¯ã‚’å‘¼ã³å‡ºã™
    # â˜… generate_title ãŒ True ã®å ´åˆã€å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«JSONæŒ‡ç¤ºã‚’è¿½åŠ ã™ã‚‹
    final_prompt = prompt
    output_parser = StrOutputParser()
    if generate_title:
        json_instruction = """
ã€é‡è¦ï¼šå‡ºåŠ›å½¢å¼ã€‘
ã‚ãªãŸã®å›ç­”ã¨ã€ã“ã®ä¼šè©±ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’è€ƒãˆã€å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚
{{
  "answer": "ã“ã“ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®å›ç­”æœ¬æ–‡ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚",
  "title": "ã“ã“ã«30æ–‡å­—ç¨‹åº¦ã®ä¼šè©±ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚"
}}"""
        final_prompt = prompt + "\n\n" + json_instruction
        output_parser = JsonOutputParser()

    # â˜… æ—¢å­˜ã® create_unified_chain ã‚’å‘¼ã³å‡ºã™ãŒã€æœ«å°¾ã®ãƒ‘ãƒ¼ã‚µãƒ¼ã ã‘ã‚’å·®ã—æ›¿ãˆã‚‹
    # ã“ã®æ–¹æ³•ã§ã¯ create_unified_chain ã®ä¸­èº«ã‚’æ›¸ãæ›ãˆã‚‹å¿…è¦ãŒã‚ã‚Šã€å…ƒã®ã‚³ãƒ¼ãƒ‰ã®å¤‰æ›´ãŒå¤§ãããªã‚‹ãŸã‚ã€
    # ã“ã“ã§ãƒã‚§ãƒ¼ãƒ³ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’å†å®šç¾©ã™ã‚‹ã®ãŒæœ€ã‚‚å®‰å…¨ã§ã™ã€‚å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ã¯å®Œå…¨ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã™ã€‚
    
    chat_model = get_chat_model(model, temperature, max_tokens)
    
    # å…ƒã® create_unified_chain ã®ä¸­èº«ã‚’ã“ã“ã«å±•é–‹
    if mode == "è³ªç–‘å¿œç­”æ›¸æ·»å‰Šãƒ¢ãƒ¼ãƒ‰":
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", final_prompt),
            MessagesPlaceholder(variable_name="chat_history", optional=True),
            ("human", "ã€æ·»å‰Šä¾é ¼ã€‘\n{question}\n\nä¸Šè¨˜ã®å†…å®¹ã«ã¤ã„ã¦ã€è³ªç–‘å¿œç­”æ›¸ã¨ã—ã¦é©åˆ‡ãªå½¢å¼ã§æ·»å‰Šãƒ»æ”¹å–„ææ¡ˆã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚")
        ])
        chain = (
            {
                "question": lambda x: x["question"],
                "chat_history": lambda x: ChainManager.create_chat_history_messages(x.get("chat_history"))
            }
            | prompt_template
            | chat_model
            | output_parser
        )
    else: # æš—é»™çŸ¥æ³•ä»¤ã€ãƒ“ãƒ«ãƒã‚¹ã‚¿ã€ãã®ä»–ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ¼ãƒ‰
        # ã©ã®ãƒ¢ãƒ¼ãƒ‰ã§ã‚‚ knowledge_generator ã‚’ä½¿ã†æƒ³å®šã§æ±ç”¨åŒ–
        if mode == "æš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰":
            prompt_template = ChatPromptTemplate.from_messages([
                ("system", final_prompt),
                ("human", "=== è¨­å‚™è³‡æ–™æƒ…å ± ===\n{equipment_content}\n\n=== ãƒ“ãƒ«æƒ…å ± ===\n{building_content}"),
                MessagesPlaceholder(variable_name="chat_history", optional=True),
                ("human", "ã€æŠ€è¡“çš„è³ªå•ã€‘\n{question}\n\nä¸Šè¨˜ã®è¨­å‚™è³‡æ–™ã¨ãƒ“ãƒ«æƒ…å ±ã‚’å‚è€ƒã«ã€å»ºç¯‰é›»æ°—è¨­å‚™è¨­è¨ˆã®è¦³ç‚¹ã‹ã‚‰è©³ç´°ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚")
            ])
            knowledge_generator = RunnableLambda(ChainManager.create_separate_knowledge)
        elif mode == "ãƒ“ãƒ«ãƒã‚¹ã‚¿è³ªå•ãƒ¢ãƒ¼ãƒ‰":
            prompt_template = ChatPromptTemplate.from_messages([
                ("system", final_prompt),
                ("human", "=== ãƒ“ãƒ«ãƒã‚¹ã‚¿ãƒ¼æƒ…å ± ===\n{building_content}"),
                MessagesPlaceholder(variable_name="chat_history", optional=True),
                ("human", "ã€ãƒ“ãƒ«æƒ…å ±ã«é–¢ã™ã‚‹è³ªå•ã€‘\n{question}\n\nãƒ“ãƒ«ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹æƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ã€æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚")
            ])
            knowledge_generator = RunnableLambda(ChainManager.create_building_knowledge)
        else: # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            prompt_template = ChatPromptTemplate.from_messages([
                ("system", final_prompt),
                ("human", "{knowledge_contents}"),
                MessagesPlaceholder(variable_name="chat_history", optional=True),
                ("human", "ã€è³ªå•ã€‘\n{question}\n\nä¸Šè¨˜ã®è³‡æ–™æƒ…å ±ã‚’å‚è€ƒã«ã€æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚")
            ])
            knowledge_generator = RunnableLambda(ChainManager.create_combined_knowledge)
        
        chain = (
            {
                "question": lambda x: x["question"],
                "equipment_content": lambda x: x.get("equipment_content", ""),
                "building_content": lambda x: x.get("building_content", ""),
                "target_building_content": lambda x: x.get("target_building_content", ""),
                "other_buildings_content": lambda x: x.get("other_buildings_content", ""),
                "knowledge_contents": knowledge_generator,
                "chat_history": lambda x: ChainManager.create_chat_history_messages(x.get("chat_history"))
            }
            | prompt_template
            | chat_model
            | output_parser
        )

    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿æº–å‚™
    chain_input = {
        "question": question,
        "chat_history": chat_history[:-1] if chat_history and len(chat_history) > 1 else None,
        "equipment_content": equipment_content or "",
        "building_content": building_content or "",
        "target_building_content": target_building_content or "",
        "other_buildings_content": other_buildings_content or ""
    }

    try:
        actual_complete_prompt = get_actual_prompt_from_template(
            prompt_template, chain_input, mode
        )
    except Exception as e:
        logger.error(f"âŒ Prompt extraction failed: {e}")
        actual_complete_prompt = f"=== SYSTEM ===\n{final_prompt}\n\n=== HUMAN ===\n{question}"
    
    # ãƒã‚§ãƒ¼ãƒ³å®Ÿè¡Œã¨çµæœã®æ•´å½¢
    try:
        response = chain.invoke(chain_input)
        
        # complete_prompt ã®æ§‹ç¯‰ãƒ­ã‚¸ãƒƒã‚¯ã¯å…ƒã®ã‚³ãƒ¼ãƒ‰ã‹ã‚‰çœç•¥ï¼ˆå¿…è¦ãªã‚‰å¾Œã§å¾©æ´»å¯èƒ½ï¼‰
        
        if generate_title:
            return {
                "answer": response.get("answer", "å¿œç­”ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"),
                "title": response.get("title"),
                "langchain_used": True,
                "complete_prompt": actual_complete_prompt
            }
        else:
            return {
                "answer": str(response),
                "title": None,
                "langchain_used": True,
                "complete_prompt": actual_complete_prompt
            }
        
    except Exception as e:
        logger.error(f"âŒ çµ±ä¸€å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        # æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã«åˆã‚ã›ã¦ã‚¨ãƒ©ãƒ¼ã‚’å†ç™ºç”Ÿã•ã›ã‚‹
        raise

def generate_smart_answer_with_langchain(
    *,
    prompt: str,
    question: str,
    model: str = "claude-4-sonnet",
    mode: str = "æš—é»™çŸ¥æ³•ä»¤ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰",
    equipment_content: Optional[str] = None,
    building_content: Optional[str] = None,
    target_building_content: Optional[str] = None,
    other_buildings_content: Optional[str] = None,
    chat_history: Optional[List[Dict[str, str]]] = None,
    temperature: float = 0.0,
    max_tokens: Optional[int] = None,
    generate_title: bool = False # â˜…app.pyã‹ã‚‰æ¸¡ã•ã‚Œã‚‹ãƒ•ãƒ©ã‚°
) -> Dict[str, Any]:
    """
    æ—¢å­˜ã®app.pyã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹é–¢æ•°ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
    â˜… generate_title ãƒ•ãƒ©ã‚°ã‚’ä¸‹ã®é–¢æ•°ã«æ¸¡ã™å½¹å‰²ã‚’è¿½åŠ 
    """
    # æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã§ã¯ generate_unified_answer ã‚’å‘¼ã³å‡ºã—ã¦ã„ã‚‹ã®ã§ã€ãã®æ§‹é€ ã‚’ç¶­æŒ
    # generate_title ãƒ•ãƒ©ã‚°ã‚’æ¸¡ã™ã‚ˆã†ã«å¤‰æ›´
    response_dict = generate_unified_answer(
        prompt=prompt,
        question=question,
        model=model,
        mode=mode,
        equipment_content=equipment_content,
        building_content=building_content,
        target_building_content=target_building_content,
        other_buildings_content=other_buildings_content,
        chat_history=chat_history,
        temperature=temperature,
        max_tokens=max_tokens,
        generate_title=generate_title # â˜…ãƒ•ãƒ©ã‚°ã‚’æ¸¡ã™
    )
    
    # æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã¯ generate_unified_answer ã®æˆ»ã‚Šå€¤ã‚’ãã®ã¾ã¾è¿”ã—ã¦ã„ãŸã®ã§ã€
    # ãã®æ§‹é€ ã‚’æ¨¡å€£ã™ã‚‹ãŒã€æ–°ã—ã„ã‚­ãƒ¼ 'title' ã‚’å«ã‚ã‚‹
    return response_dict

# =================================================================
# â–¼ å¤‰æ›´ç‚¹
# ã“ã®é–¢æ•°ã¯ä¸è¦ã«ãªã‚‹ãŸã‚ã€å®Œå…¨ã«å‰Šé™¤ã—ã¾ã™ã€‚
# =================================================================
# def generate_chat_title_with_llm(...):

# =================================================================
# â–¼ å¤‰æ›´ç‚¹
# ã“ã®ãƒ†ã‚¹ãƒˆé–¢æ•°ã¯å¤ã„æ§‹æˆã«åŸºã¥ã„ã¦ã„ã‚‹ãŸã‚ã€ä¸€æ—¦ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã™ã‚‹ã‹å‰Šé™¤ã—ã¾ã™ã€‚
# =================================================================
# def test_chain_creation():
# if __name__ == "__main__":
#     test_chain_creation()