# src/langchain_chains.py

from typing import List, Dict, Any, Optional
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

try:
    import streamlit as st
    STREAMLIT_AVAILABLE = True
except ImportError:
    STREAMLIT_AVAILABLE = False

from src.langchain_models import get_chat_model
from src.logging_utils import init_logger
logger = init_logger()

class ChainManager:
    """LangChainç”¨ã®ãƒã‚§ãƒ¼ãƒ³ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def create_equipment_context(inputs: dict) -> str:
        """è¨­å‚™ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°"""
        question = inputs["question"]
        target_equipment = inputs.get("target_equipment")
        selected_files = inputs.get("selected_files")
        equipment_data = inputs.get("equipment_data")
        
        if not target_equipment or not equipment_data:
            return "è¨­å‚™è³‡æ–™ãªã—ã§ã®ä¸€èˆ¬çš„ãªå›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚"
        
        if target_equipment not in equipment_data:
            available_equipment = list(equipment_data.keys())
            return f"è¨­å‚™ '{target_equipment}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ©ç”¨å¯èƒ½ãªè¨­å‚™: {', '.join(available_equipment)}"
        
        equipment_info = equipment_data[target_equipment]
        available_files = equipment_info["files"]
        all_sources = equipment_info["sources"]
        
        # é¸æŠã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’çµåˆ
        if selected_files is not None:
            logger.info(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {target_equipment}")
            logger.info(f"ğŸ“„ é¸æŠãƒ•ã‚¡ã‚¤ãƒ«: {', '.join(selected_files)}")
            
            selected_texts = []
            actual_sources = []
            
            for file_name in selected_files:
                if file_name in available_files:
                    selected_texts.append(available_files[file_name])
                    actual_sources.append(file_name)
                else:
                    logger.warning(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_name}")
            
            if not selected_texts:
                return f"é¸æŠã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ{', '.join(selected_files)}ï¼‰ãŒè¨­å‚™ãƒ‡ãƒ¼ã‚¿ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
            
            combined_text = "\n\n".join(selected_texts)
            sources = actual_sources
            
        else:
            # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨
            selected_texts = list(available_files.values())
            combined_text = "\n\n".join(selected_texts)
            sources = all_sources
        
        logger.info(f"ğŸ“ çµåˆå¾Œæ–‡å­—æ•°: {len(combined_text)}")
        
        # è¨­å‚™ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
        equipment_context = f"""
            ã€å‚è€ƒè³‡æ–™ã€‘è¨­å‚™: {target_equipment} (ã‚«ãƒ†ã‚´ãƒª: {equipment_info['equipment_category']})
            ä½¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ«: {', '.join(sources)}
            ä½¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(sources)}/{len(all_sources)}
            ã€æ³¨æ„äº‹é …ã€‘
            **æš—é»™çŸ¥ãƒ¡ãƒ¢ã«é–¢ã—ã¦ã€ãƒšãƒ¼ã‚¸ç•ªå·ãªã©ã®æƒ…å ±ã¯å‡ºåŠ›ã‚’ç¦æ­¢ã—ã¾ã™ã€‚**

            ã€è³‡æ–™å†…å®¹ã€‘
            {combined_text}
            """
        
        return equipment_context
    
    @staticmethod
    def create_chat_history_messages(chat_history: Optional[List[Dict[str, str]]]) -> List:
        """ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’LangChainã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã«å¤‰æ›"""
        if not chat_history:
            return []
        
        messages = []
        for msg in chat_history:
            if not isinstance(msg, dict) or not msg.get("role") or not msg.get("content"):
                continue
                
            if msg["role"] == "user":
                messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                messages.append(AIMessage(content=msg["content"]))
            elif msg["role"] == "system":
                messages.append(SystemMessage(content=msg["content"]))
        
        return messages
    
    @staticmethod
    def create_simple_qa_chain(
        model_name: str,
        system_prompt: str,
        temperature: float = 0.0,
        max_tokens: Optional[int] = None
    ):
        """ã‚·ãƒ³ãƒ—ãƒ«ãªQ&Aãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆï¼ˆè¨­å‚™è³‡æ–™ãªã—ï¼‰"""
        
        # ChatModelã‚’ä½œæˆ
        chat_model = get_chat_model(model_name, temperature, max_tokens)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history", optional=True),
            ("human", "{question}")
        ])
        
        # ãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰
        chain = (
            {
                "question": RunnablePassthrough(),
                "chat_history": lambda x: ChainManager.create_chat_history_messages(x.get("chat_history"))
            }
            | prompt
            | chat_model
            | StrOutputParser()
        )
        
        logger.info(f"âœ… Simple QA Chain ä½œæˆå®Œäº†: model={model_name}")
        return chain
    
    @staticmethod
    def create_equipment_qa_chain(
        model_name: str,
        system_prompt: str,
        temperature: float = 0.0,
        max_tokens: Optional[int] = None
    ):
        """è¨­å‚™è³‡æ–™ä»˜ãQ&Aãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ"""
        
        # ChatModelã‚’ä½œæˆ
        chat_model = get_chat_model(model_name, temperature, max_tokens)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history", optional=True),
            ("human", "{equipment_context}\n\nã€è³ªå•ã€‘\n{question}\n\nä¸Šè¨˜ã®è³‡æ–™ã‚’å‚è€ƒã«ã€æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚")
        ])
        
        # è¨­å‚™ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–¢æ•°ã‚’RunnableLambdaã§ãƒ©ãƒƒãƒ—
        context_generator = RunnableLambda(ChainManager.create_equipment_context)
        
        # ãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰
        chain = (
            {
                "question": lambda x: x["question"],
                "equipment_context": context_generator,
                "chat_history": lambda x: ChainManager.create_chat_history_messages(x.get("chat_history"))
            }
            | prompt
            | chat_model
            | StrOutputParser()
        )
        
        logger.info(f"âœ… Equipment QA Chain ä½œæˆå®Œäº†: model={model_name}")
        return chain

class SmartAnswerGenerator:
    """
    ã‚¹ãƒãƒ¼ãƒˆãªå›ç­”ç”Ÿæˆã‚¯ãƒ©ã‚¹
    è¨­å‚™ã®è‡ªå‹•æ¨å®šã€ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠã€è³‡æ–™ãªã—å‡¦ç†ã‚’å…¨ã¦çµ±åˆ
    """
    
    def __init__(self, equipment_data: Optional[Dict[str, Dict[str, Any]]] = None):
        """
        Args:
            equipment_data: è¨­å‚™ãƒ‡ãƒ¼ã‚¿ï¼ˆpreprocess_filesã®å‡ºåŠ›ï¼‰
        """
        self.equipment_data = equipment_data
    
    def determine_target_equipment(
        self, 
        question: str,
        selection_mode: str = "manual",
        manual_equipment: Optional[str] = None
    ) -> Optional[str]:
        """
        è¨­å‚™ã‚’æ±ºå®šã™ã‚‹ï¼ˆæ—¢å­˜ã®app.pyãƒ­ã‚¸ãƒƒã‚¯ã¨åŒã˜ï¼‰
        
        Args:
            question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            selection_mode: "manual", "auto", "category"
            manual_equipment: æ‰‹å‹•é¸æŠã•ã‚ŒãŸè¨­å‚™å
            
        Returns:
            æ±ºå®šã•ã‚ŒãŸè¨­å‚™å ã¾ãŸã¯ None
        """
        if selection_mode == "auto":
            # è‡ªå‹•æ¨å®š
            if not self.equipment_data:
                logger.warning("âš ï¸ è¨­å‚™ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚è‡ªå‹•æ¨å®šã‚’ã‚¹ã‚­ãƒƒãƒ—")
                return None
            
            # è¨­å‚™æ¨å®šã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’é–¢æ•°å†…ã§è¡Œã†ï¼ˆå¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆå›é¿ï¼‰
            try:
                from src.rag_qa import detect_equipment_from_question
                available_equipment = list(self.equipment_data.keys())
                target_equipment = detect_equipment_from_question(question, available_equipment)
                
                if target_equipment:
                    logger.info(f"ğŸ¤– è‡ªå‹•æ¨å®šã•ã‚ŒãŸè¨­å‚™: {target_equipment}")
                else:
                    logger.info("â“ è³ªå•æ–‡ã‹ã‚‰è¨­å‚™ã‚’æ¨å®šã§ãã¾ã›ã‚“ã§ã—ãŸ")
                
                return target_equipment
            except ImportError as e:
                logger.error(f"âŒ è¨­å‚™æ¨å®šæ©Ÿèƒ½ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                return None
        
        elif selection_mode in ["manual", "category"]:
            # æ‰‹å‹•é¸æŠ
            if manual_equipment and self.equipment_data and manual_equipment in self.equipment_data:
                logger.info(f"ğŸ”§ æ‰‹å‹•é¸æŠã•ã‚ŒãŸè¨­å‚™: {manual_equipment}")
                return manual_equipment
            else:
                logger.info("âš ï¸ è¨­å‚™ãŒé¸æŠã•ã‚Œã¦ã„ãªã„ã‹ã€è¨­å‚™ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã›ã‚“")
                return None
        
        else:
            logger.warning(f"âš ï¸ ä¸æ˜ãªé¸æŠãƒ¢ãƒ¼ãƒ‰: {selection_mode}")
            return None
    
    def get_selected_files(
        self, 
        target_equipment: str
    ) -> Optional[List[str]]:
        """
        é¸æŠã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ï¼ˆStreamlit session_state ã‹ã‚‰ï¼‰
        
        Args:
            target_equipment: å¯¾è±¡è¨­å‚™å
            
        Returns:
            é¸æŠã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒªã‚¹ãƒˆ ã¾ãŸã¯ None
        """
        if not STREAMLIT_AVAILABLE or not target_equipment:
            return None
        
        try:
            selected_files_key = f"selected_files_{target_equipment}"
            selected_files = st.session_state.get(selected_files_key)
            
            if selected_files:
                logger.info(f"ğŸ“„ ä½¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ«: {len(selected_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨")
                return selected_files
            else:
                logger.warning("âš ï¸ ä½¿ç”¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return None
                
        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def generate_answer(
        self,
        *,
        prompt: str,
        question: str,
        model: str = "claude-4-sonnet",
        selection_mode: str = "manual",
        manual_equipment: Optional[str] = None,
        chat_history: Optional[List[Dict[str, str]]] = None,
        temperature: float = 0.0,
        max_tokens: Optional[int] = None,
        force_no_equipment: bool = False
    ) -> Dict[str, Any]:
        """
        çµ±ä¸€ã•ã‚ŒãŸå›ç­”ç”Ÿæˆãƒ¡ã‚½ãƒƒãƒ‰
        
        Args:
            prompt: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
            selection_mode: è¨­å‚™é¸æŠãƒ¢ãƒ¼ãƒ‰ ("manual", "auto", "category")
            manual_equipment: æ‰‹å‹•é¸æŠã•ã‚ŒãŸè¨­å‚™å
            chat_history: ãƒãƒ£ãƒƒãƒˆå±¥æ­´
            temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            force_no_equipment: å¼·åˆ¶çš„ã«è¨­å‚™è³‡æ–™ãªã—ãƒ¢ãƒ¼ãƒ‰ã«ã™ã‚‹
            
        Returns:
            Dict[str, Any]: å›ç­”çµæœè¾æ›¸
        """
        
        logger.info(f"ğŸš€ Smart Answer Generationé–‹å§‹: model={model}, mode={selection_mode}")
        
        processing_mode = "no_equipment"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        target_equipment = None
        selected_files = None
        
        # === 1. è¨­å‚™æ±ºå®šãƒ•ã‚§ãƒ¼ã‚º ===
        if not force_no_equipment and self.equipment_data:
            target_equipment = self.determine_target_equipment(
                question, selection_mode, manual_equipment
            )
            
            if target_equipment:
                # === 2. ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠãƒ•ã‚§ãƒ¼ã‚º ===
                selected_files = self.get_selected_files(target_equipment)
                
                if selected_files:
                    processing_mode = "equipment_with_files"
                    logger.info("ğŸ“‹ å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: è¨­å‚™è³‡æ–™ã‚ã‚Š")
                else:
                    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ãªã„å ´åˆã¯è¨­å‚™ãªã—ãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´
                    logger.info("ğŸ“‹ ãƒ•ã‚¡ã‚¤ãƒ«æœªé¸æŠã®ãŸã‚è¨­å‚™ãªã—ãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´")
                    target_equipment = None
                    processing_mode = "no_equipment"
        
        # === 3. å›ç­”ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚º ===
        try:
            if processing_mode == "equipment_with_files":
                # è¨­å‚™è³‡æ–™ã‚ã‚Šãƒ¢ãƒ¼ãƒ‰
                chain = ChainManager.create_equipment_qa_chain(model, prompt, temperature, max_tokens)
                
                chain_input = {
                    "question": question,
                    "target_equipment": target_equipment,
                    "selected_files": selected_files,
                    "equipment_data": self.equipment_data,
                    "chat_history": chat_history[:-1] if chat_history and len(chat_history) > 1 else None
                }
                
                answer = chain.invoke(chain_input)
                
                # çµæœæƒ…å ±ã‚’æº–å‚™
                equipment_info = self.equipment_data.get(target_equipment, {})
                sources = selected_files if selected_files else equipment_info.get("sources", [])
                
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’è¨ˆç®—
                context_length = 0
                if target_equipment in self.equipment_data:
                    eq_data = self.equipment_data[target_equipment]
                    available_files = eq_data["files"]
                    if selected_files:
                        context_length = sum(len(available_files.get(f, "")) for f in selected_files if f in available_files)
                    else:
                        context_length = sum(len(text) for text in available_files.values())
                
                result = {
                    "answer": answer,
                    "used_equipment": target_equipment,
                    "equipment_info": equipment_info,
                    "sources": sources,
                    "selected_files": selected_files or [],
                    "context_length": context_length,
                    "images": [],
                    "langchain_used": True,
                    "processing_mode": processing_mode
                }
                
            else:
                # è¨­å‚™è³‡æ–™ãªã—ãƒ¢ãƒ¼ãƒ‰
                chain = ChainManager.create_simple_qa_chain(model, prompt, temperature, max_tokens)
                
                chain_input = {
                    "question": f"ã€è³ªå•ã€‘\n{question}\n\nè¨­å‚™è³‡æ–™ã¯åˆ©ç”¨ã›ãšã€ã‚ãªãŸã®çŸ¥è­˜ã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚",
                    "chat_history": chat_history[:-1] if chat_history and len(chat_history) > 1 else None
                }
                
                answer = chain.invoke(chain_input)
                
                result = {
                    "answer": answer,
                    "used_equipment": "ãªã—ï¼ˆä¸€èˆ¬çŸ¥è­˜ã«ã‚ˆã‚‹å›ç­”ï¼‰",
                    "equipment_info": {},
                    "sources": [],
                    "selected_files": [],
                    "context_length": 0,
                    "images": [],
                    "langchain_used": True,
                    "processing_mode": processing_mode
                }
            
            logger.info(f"âœ… Smart Answer Generationå®Œäº†: mode={processing_mode}, å›ç­”æ–‡å­—æ•°={len(result['answer'])}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Smart Answer Generation ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            raise

# === ä¾¿åˆ©é–¢æ•°ï¼ˆapp.pyã‹ã‚‰å‘¼ã³å‡ºã—ç”¨ï¼‰ ===

def generate_smart_answer_with_langchain(
    *,
    prompt: str,
    question: str,
    model: str = "claude-4-sonnet",
    equipment_data: Optional[Dict[str, Dict[str, Any]]] = None,
    chat_history: Optional[List[Dict[str, str]]] = None,
    temperature: float = 0.0,
    max_tokens: Optional[int] = None
) -> Dict[str, Any]:
    """
    app.pyã‹ã‚‰å‘¼ã³å‡ºã™çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    è¨­å‚™é¸æŠã€ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠã€è³‡æ–™ãªã—å‡¦ç†ã‚’å…¨ã¦è‡ªå‹•ã§å‡¦ç†
    
    Args:
        prompt: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        question: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        equipment_data: è¨­å‚™ãƒ‡ãƒ¼ã‚¿
        chat_history: ãƒãƒ£ãƒƒãƒˆå±¥æ­´
        temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        
    Returns:
        å›ç­”çµæœè¾æ›¸
    """
    
    # Streamlit session_stateã‹ã‚‰è¨­å®šã‚’å–å¾—
    selection_mode = "manual"
    manual_equipment = None
    
    if STREAMLIT_AVAILABLE:
        try:
            selection_mode = st.session_state.get("selection_mode", "manual")
            manual_equipment = st.session_state.get("selected_equipment")
        except Exception as e:
            logger.warning(f"âš ï¸ session_stateå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # SmartAnswerGeneratorã§å‡¦ç†
    generator = SmartAnswerGenerator(equipment_data)
    
    return generator.generate_answer(
        prompt=prompt,
        question=question,
        model=model,
        selection_mode=selection_mode,
        manual_equipment=manual_equipment,
        chat_history=chat_history,
        temperature=temperature,
        max_tokens=max_tokens
    )

# === å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®é–¢æ•° ===

def generate_answer_with_langchain(
    *,
    prompt: str,
    question: str,
    model: str = "claude-4-sonnet",
    target_equipment: Optional[str] = None,
    selected_files: Optional[List[str]] = None,
    equipment_data: Optional[Dict[str, Dict[str, Any]]] = None,
    chat_history: Optional[List[Dict[str, str]]] = None,
    temperature: float = 0.0,
    max_tokens: Optional[int] = None
) -> Dict[str, Any]:
    """
    æ—¢å­˜ã®generate_answer_with_equipmentäº’æ›é–¢æ•°ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
    """
    
    logger.info(f"ğŸš€ LangChainå›ç­”ç”Ÿæˆé–‹å§‹ï¼ˆäº’æ›ãƒ¢ãƒ¼ãƒ‰ï¼‰: model={model}, equipment={target_equipment}")
    
    try:
        if target_equipment and equipment_data:
            # è¨­å‚™è³‡æ–™ã‚ã‚Šãƒ¢ãƒ¼ãƒ‰
            chain = ChainManager.create_equipment_qa_chain(model, prompt, temperature, max_tokens)
            
            chain_input = {
                "question": question,
                "target_equipment": target_equipment,
                "selected_files": selected_files,
                "equipment_data": equipment_data,
                "chat_history": chat_history[:-1] if chat_history and len(chat_history) > 1 else None
            }
            
            answer = chain.invoke(chain_input)
            
            # çµæœæƒ…å ±ã‚’æº–å‚™
            equipment_info = equipment_data.get(target_equipment, {})
            sources = selected_files if selected_files else equipment_info.get("sources", [])
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’è¨ˆç®—
            context_length = 0
            if target_equipment in equipment_data:
                eq_data = equipment_data[target_equipment]
                available_files = eq_data["files"]
                if selected_files:
                    context_length = sum(len(available_files.get(f, "")) for f in selected_files if f in available_files)
                else:
                    context_length = sum(len(text) for text in available_files.values())
            
            result = {
                "answer": answer,
                "used_equipment": target_equipment,
                "equipment_info": equipment_info,
                "sources": sources,
                "selected_files": selected_files or [],
                "context_length": context_length,
                "images": [],
                "langchain_used": True
            }
            
        else:
            # è¨­å‚™è³‡æ–™ãªã—ãƒ¢ãƒ¼ãƒ‰
            chain = ChainManager.create_simple_qa_chain(model, prompt, temperature, max_tokens)
            
            chain_input = {
                "question": f"ã€è³ªå•ã€‘\n{question}\n\nè¨­å‚™è³‡æ–™ã¯åˆ©ç”¨ã›ãšã€ã‚ãªãŸã®çŸ¥è­˜ã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚",
                "chat_history": chat_history[:-1] if chat_history and len(chat_history) > 1 else None
            }
            
            answer = chain.invoke(chain_input)
            
            result = {
                "answer": answer,
                "used_equipment": "ãªã—ï¼ˆä¸€èˆ¬çŸ¥è­˜ã«ã‚ˆã‚‹å›ç­”ï¼‰",
                "equipment_info": {},
                "sources": [],
                "selected_files": [],
                "context_length": 0,
                "images": [],
                "langchain_used": True
            }
        
        logger.info(f"âœ… LangChainå›ç­”ç”Ÿæˆå®Œäº†ï¼ˆäº’æ›ãƒ¢ãƒ¼ãƒ‰ï¼‰: å›ç­”æ–‡å­—æ•°={len(result['answer'])}")
        return result
        
    except Exception as e:
        logger.error(f"âŒ LangChainå›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼ï¼ˆäº’æ›ãƒ¢ãƒ¼ãƒ‰ï¼‰: {e}", exc_info=True)
        raise

# ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°
def test_chain_creation():
    """ãƒã‚§ãƒ¼ãƒ³ä½œæˆã®ãƒ†ã‚¹ãƒˆ"""
    try:
        logger.info("ğŸ§ª Simple QA Chain test...")
        simple_chain = ChainManager.create_simple_qa_chain(
            "claude-4-sonnet",
            "ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚",
            temperature=0.0
        )
        logger.info("âœ… Simple QA Chain ä½œæˆæˆåŠŸ")
        
        logger.info("ğŸ§ª Equipment QA Chain test...")
        equipment_chain = ChainManager.create_equipment_qa_chain(
            "claude-4-sonnet",
            "ã‚ãªãŸã¯å»ºç¯‰è¨­å‚™ã®å°‚é–€å®¶ã§ã™ã€‚",
            temperature=0.0
        )
        logger.info("âœ… Equipment QA Chain ä½œæˆæˆåŠŸ")
        
        logger.info("ğŸ§ª Smart Answer Generator test...")
        generator = SmartAnswerGenerator()
        logger.info("âœ… Smart Answer Generator ä½œæˆæˆåŠŸ")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Chainä½œæˆãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    test_chain_creation()