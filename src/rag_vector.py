# src/rag_vector.py
from __future__ import annotations
from io import BytesIO
from typing import List, Dict, Any
import uuid

# SQLite ãƒãƒ¼ã‚¸ãƒ§ãƒ³å¼·åˆ¶ä¸Šæ›¸ãï¼ˆpysqlite3ã§chromadbãŒä½¿ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼‰
import sys
import pysqlite3
sys.modules["sqlite3"] = pysqlite3

import chromadb
from chromadb.config import Settings
from openai import OpenAI
from tenacity import retry, wait_random_exponential, stop_after_attempt

# from transformers import CLIPProcessor, CLIPModel
# from PIL import Image
# import torch
from uuid import uuid4

__all__ = ["save_docs_to_chroma", "query_collection"]

# ---------------------------------------------------------------------------
# ãƒ¢ãƒ‡ãƒ«ï¼†ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
# ---------------------------------------------------------------------------
_OPENAI_MODEL = "text-embedding-ada-002"
_openai_client = OpenAI()

# _clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
# _clip_proc  = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# ---------------------------------------------------------------------------
# ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ï¼ˆãƒãƒƒãƒï¼‰
# ---------------------------------------------------------------------------
@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))
def _embed_text_batch(texts: List[str]) -> List[List[float]]:
    resp = _openai_client.embeddings.create(model=_OPENAI_MODEL, input=texts)
    return [item.embedding for item in resp.data]

# # ---------------------------------------------------------------------------
# # ç”»åƒåŸ‹ã‚è¾¼ã¿ï¼ˆCLIPï¼‰
# # ---------------------------------------------------------------------------
# def _embed_image(img_bytes: bytes) -> List[float]:
#     img = Image.open(BytesIO(img_bytes)).convert("RGB")
#     inputs = _clip_proc(images=img, return_tensors="pt")
#     with torch.no_grad():
#         feats = _clip_model.get_image_features(**inputs)
#     feats = feats / feats.norm(p=2, dim=-1, keepdim=True)
#     return feats[0].cpu().tolist()

# ---------------------------------------------------------------------------
# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ ChromaDB ã«ä¿å­˜
# ---------------------------------------------------------------------------
def save_docs_to_chroma(
        *,
        docs: List[Dict[str, Any]],
        collection_name: str,
        persist_directory: str | None = None,
        batch_size: int = 50,
    ) -> chromadb.api.Collection:
        """
        1) ChromaDB ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆãƒ»å–å¾—
        2) preprocess_files å‡ºåŠ› docs ã‚’ãƒãƒƒãƒç™»éŒ² (é‡è¤‡é™¤å»ä»˜ã)
        3) å¿…è¦ãªã‚‰æ°¸ç¶šåŒ–ã—ã€Collection ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™
        """
        # â€” ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ â€”
        if persist_directory:
            client = chromadb.Client(Settings(persist_directory=persist_directory))
        else:
            client = chromadb.Client()  # ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒª

        # â€” æ—¢å­˜ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤ã—ã¦æ–°è¦ä½œæˆ â€”
        try:
            client.delete_collection(name=collection_name)
            print(f"ğŸ—‘ï¸ æ—¢å­˜ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å‰Šé™¤: {collection_name}")
        except ValueError:
            print(f"ğŸ“ æ–°è¦ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆ: {collection_name}")

        collection = client.create_collection(name=collection_name)

        # â€” docs ã‚’ãƒãƒƒãƒç™»éŒ² â€”
        docs_to_index = [d for d in docs if d["metadata"].get("kind") in ("text", "table")]
        print(f"ğŸ” å‡¦ç†å¯¾è±¡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {len(docs_to_index)}")
        
        # ğŸ”¥ å…¨ä½“ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯IDã‚’ç®¡ç†
        global_seen_ids = set()
        processed_docs = []
        
        for doc in docs_to_index:
            metadata = doc["metadata"]
            source = metadata.get('source', 'unknown')
            kind = metadata.get('kind', 'unknown')
            chunk_id = metadata.get('chunk_id', metadata.get('table_id', 0))
            page = metadata.get('page', 'unknown')
            
            # ã‚ˆã‚Šè©³ç´°ãªIDç”Ÿæˆ
            doc_id = f"{source}-{kind}-p{page}-c{chunk_id}"
            
            # ãƒãƒƒãƒã‚’è·¨ã„ã é‡è¤‡ãƒã‚§ãƒƒã‚¯
            if doc_id not in global_seen_ids:
                global_seen_ids.add(doc_id)
                processed_docs.append((doc, doc_id))
            else:
                print(f"âš ï¸ é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {doc_id}")
        
        print(f"âœ… é‡è¤‡é™¤å»å¾Œ: {len(processed_docs)} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
        
        # ãƒãƒƒãƒå‡¦ç†
        for start in range(0, len(processed_docs), batch_size):
            batch = processed_docs[start:start+batch_size]
            
            embeddings, documents, metadatas, ids = [], [], [], []
            batch_seen_ids = set()  # ãƒãƒƒãƒå†…é‡è¤‡ãƒã‚§ãƒƒã‚¯
            
            for doc, doc_id in batch:
                # ãƒãƒƒãƒå†…ã§ã‚‚é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆå¿µã®ãŸã‚ï¼‰
                if doc_id not in batch_seen_ids:
                    try:
                        emb = _embed_text_batch([doc["content"]])[0]
                        embeddings.append(emb)
                        documents.append(doc["content"])
                        metadatas.append(doc["metadata"])
                        ids.append(doc_id)
                        batch_seen_ids.add(doc_id)
                        
                    except Exception as e:
                        print(f"âŒ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼ (ID: {doc_id}): {e}")
                else:
                    print(f"âš ï¸ ãƒãƒƒãƒå†…é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {doc_id}")
            
            # ãƒãƒƒãƒã‚’ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«è¿½åŠ 
            if embeddings:
                try:
                    collection.add(
                        embeddings=embeddings,
                        documents=documents,
                        metadatas=metadatas,
                        ids=ids
                    )
                    print(f"âœ… ãƒãƒƒãƒ {start//batch_size + 1} å®Œäº†: {len(embeddings)} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
                    
                except Exception as e:
                    print(f"âŒ ãƒãƒƒãƒè¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
                    # å€‹åˆ¥è¿½åŠ ã‚’è©¦è¡Œ
                    for i, (emb, doc, meta, doc_id) in enumerate(zip(embeddings, documents, metadatas, ids)):
                        try:
                            collection.add(
                                embeddings=[emb],
                                documents=[doc],
                                metadatas=[meta],
                                ids=[doc_id]
                            )
                        except Exception as e2:
                            print(f"âŒ å€‹åˆ¥è¿½åŠ å¤±æ•— (ID: {doc_id}): {e2}")
            
        # æ°¸ç¶šåŒ–
        if persist_directory:
            try:
                client.persist()
            except Exception as e:
                print(f"âš ï¸ æ°¸ç¶šåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        final_count = collection.count()
        print(f"ğŸ¯ æœ€çµ‚ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ•°: {final_count}")
        
        return collection

# ---------------------------------------------------------------------------
# ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ¤œç´¢ãƒ˜ãƒ«ãƒ‘ãƒ¼
# ---------------------------------------------------------------------------
def query_collection(
    collection: chromadb.api.Collection,
    query: str,
    n_results: int = 5,
) -> List[Dict[str, Any]]:

    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚¯ã‚¨ãƒªã‚’åŸ‹ã‚è¾¼ã¿æ¤œç´¢ã—ã€ä¸Šä½ n_results ä»¶ã‚’ dict ã®ãƒªã‚¹ãƒˆã§è¿”ã™ã€‚
    """
    # 1) ã‚¯ã‚¨ãƒªã‚’åŸ‹ã‚è¾¼ã¿
    q_emb = _embed_text_batch([query])[0]
    # 2) æ¤œç´¢å®Ÿè¡Œ
    res = collection.query(
        query_embeddings=[q_emb],
        n_results=n_results,
    )
    # 3) çµæœã‚’æŠ½å‡º
    documents = res["documents"][0]
    metadatas = res["metadatas"][0]
    distances = res["distances"][0]
    # 4) dict ãƒªã‚¹ãƒˆã«æ•´å½¢
    hits: List[Dict[str, Any]] = []
    for doc, meta, dist in zip(documents, metadatas, distances):
        hits.append({
            "content": doc,
            "metadata": meta,
            "distance": dist,
        })
    return hits