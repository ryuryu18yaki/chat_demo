import streamlit as st
from pathlib import Path
from src.rag_preprocess import preprocess_files
from src.rag_vector import save_docs_to_chroma
import hashlib

def initialize_chroma_from_input_debug(input_dir: str, persist_dir: str | None, collection_name: str = "default") -> dict:
    """
    ãƒ™ã‚¯ãƒˆãƒ«DBã‚’æ§‹ç¯‰ã—ã€collection ã¨ PDFãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã‚’è¿”ã™ï¼ˆStreamlitè¡¨ç¤ºç‰ˆï¼‰
    """
    debug_messages = []  # ãƒ‡ãƒãƒƒã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è“„ç©
    
    debug_messages.append(f"ğŸ” RAGåˆæœŸåŒ–é–‹å§‹ - input_dir={input_dir}")
    
    input_path = Path(input_dir)
    files = list(input_path.glob("**/*.*"))
    
    debug_messages.append(f"ğŸ“ ç™ºè¦‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files)}")
    for f in files:
        debug_messages.append(f"  - {f.name} (ã‚µã‚¤ã‚º: {f.stat().st_size} bytes)")

    # PDFã‚’èª­ã¿è¾¼ã‚“ã§ doc åŒ–
    file_dicts = []
    for f in files:
        file_dict = {
            "name": f.name,
            "type": "application/pdf",
            "size": f.stat().st_size,
            "data": f.read_bytes()
        }
        file_dicts.append(file_dict)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—ã—ã¦é‡è¤‡ãƒã‚§ãƒƒã‚¯
        file_hash = hashlib.md5(file_dict["data"]).hexdigest()
        debug_messages.append(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«: {f.name}, ãƒãƒƒã‚·ãƒ¥: {file_hash[:12]}")

    debug_messages.append("ğŸ“š preprocess_filesé–‹å§‹...")
    docs = preprocess_files(file_dicts)
    debug_messages.append(f"ğŸ“š preprocess_fileså®Œäº† - ç”Ÿæˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(docs)}")
    
    # ç”Ÿæˆã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®åˆ†æ
    doc_analysis = {}
    content_hashes = {}
    duplicate_contents = []
    
    for i, doc in enumerate(docs):
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ†æ
        metadata = doc.metadata if hasattr(doc, 'metadata') else {}
        source = metadata.get('source', 'unknown')
        page = metadata.get('page', 'unknown')
        kind = metadata.get('kind', 'unknown')
        
        key = f"{source}_p{page}_{kind}"
        doc_analysis[key] = doc_analysis.get(key, 0) + 1
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é‡è¤‡ãƒã‚§ãƒƒã‚¯
        content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
        content_hash = hashlib.md5(content.encode()).hexdigest()
        
        if content_hash in content_hashes:
            duplicate_contents.append({
                'hash': content_hash,
                'original_index': content_hashes[content_hash],
                'duplicate_index': i,
                'content_preview': content[:100]
            })
            debug_messages.append(f"âš ï¸ é‡è¤‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œå‡º - ãƒãƒƒã‚·ãƒ¥: {content_hash[:12]}, ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {content_hashes[content_hash]} -> {i}")
        else:
            content_hashes[content_hash] = i
    
    # åˆ†æçµæœã‚’è“„ç©
    debug_messages.append("ğŸ“Š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ†æçµæœ:")
    for key, count in doc_analysis.items():
        if count > 1:
            debug_messages.append(f"  âš ï¸ {key}: {count}å€‹ (é‡è¤‡ã®å¯èƒ½æ€§)")
        else:
            debug_messages.append(f"  âœ… {key}: {count}å€‹")
    
    if duplicate_contents:
        debug_messages.append(f"âŒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é‡è¤‡ã‚’ {len(duplicate_contents)} ä»¶æ¤œå‡º!")
        for dup in duplicate_contents[:3]:  # æœ€åˆã®3ä»¶
            debug_messages.append(f"  - ãƒãƒƒã‚·ãƒ¥ {dup['hash'][:12]}: {dup['original_index']} -> {dup['duplicate_index']}")
    else:
        debug_messages.append("âœ… ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é‡è¤‡ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

    debug_messages.append("ğŸ’¾ save_docs_to_chromaé–‹å§‹...")
    collection = save_docs_to_chroma(
        docs=docs, 
        collection_name=collection_name, 
        persist_directory=persist_dir
    )
    debug_messages.append("ğŸ’¾ save_docs_to_chromaå®Œäº†")
    
    # æœ€çµ‚çš„ãªã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³çµ±è¨ˆ
    final_count = collection.count()
    debug_messages.append(f"ğŸ¯ æœ€çµ‚çµæœ - å…¥åŠ›docs: {len(docs)}, DBãƒãƒ£ãƒ³ã‚¯æ•°: {final_count}")
    
    if len(docs) != final_count:
        debug_messages.append("âš ï¸ å…¥åŠ›ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã¨DBä¿å­˜æ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“!")

    return {
        "collection": collection,
        "rag_files": file_dicts,
        "debug_info": {
            "input_docs": len(docs),
            "final_chunks": final_count,
            "duplicate_contents": len(duplicate_contents),
            "doc_analysis": doc_analysis,
            "debug_messages": debug_messages  # ãƒ‡ãƒãƒƒã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚‚ä¿å­˜
        }
    }

# Streamlitã‚µã‚¤ãƒ‰ãƒãƒ¼ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‡ãƒãƒƒã‚°ãƒ‘ãƒãƒ«
def render_debug_panel():
    """Streamlitã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ãƒ‡ãƒãƒƒã‚°ãƒ‘ãƒãƒ«ã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°"""
    
    st.markdown("### ğŸ”¬ RAGãƒ‡ãƒãƒƒã‚°ãƒ„ãƒ¼ãƒ«")
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤º
    if st.button("ğŸ“‹ åˆæœŸåŒ–ãƒ­ã‚°ã‚’è¡¨ç¤º"):
        if st.session_state.get("debug_info") and st.session_state.debug_info.get("debug_messages"):
            messages = st.session_state.debug_info["debug_messages"]
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’Streamlitã§è¡¨ç¤º
            st.markdown("#### ğŸ” åˆæœŸåŒ–ãƒ­ã‚°")
            
            log_container = st.container()
            with log_container:
                for msg in messages:
                    if "âš ï¸" in msg or "âŒ" in msg:
                        st.warning(msg)
                    elif "âœ…" in msg:
                        st.success(msg)
                    else:
                        st.info(msg)
                        
            # çµ±è¨ˆè¡¨ç¤º
            debug_info = st.session_state.debug_info
            st.markdown("#### ğŸ“Š çµ±è¨ˆã‚µãƒãƒªãƒ¼")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("å…¥åŠ›ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ", debug_info.get("input_docs", "N/A"))
            with col2:
                st.metric("æœ€çµ‚ãƒãƒ£ãƒ³ã‚¯æ•°", debug_info.get("final_chunks", "N/A"))
            with col3:
                st.metric("é‡è¤‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„", debug_info.get("duplicate_contents", "N/A"))
                
        else:
            st.warning("ãƒ‡ãƒãƒƒã‚°æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“")
    
    # ç°¡å˜ãªè¨ºæ–­
    if st.button("ğŸ” ç°¡å˜è¨ºæ–­"):
        if st.session_state.get("rag_collection"):
            collection = st.session_state.rag_collection
            
            try:
                # åŸºæœ¬çµ±è¨ˆ
                total_count = collection.count()
                st.info(f"ğŸ“Š ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {total_count}")
                
                # ã‚µãƒ³ãƒ—ãƒ«æ¤œç´¢ã§é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if total_count > 0:
                    all_data = collection.get(limit=10)  # æœ€åˆã®10ä»¶ã ã‘å–å¾—
                    
                    documents = all_data.get('documents', [])
                    metadatas = all_data.get('metadatas', [])
                    ids = all_data.get('ids', [])
                    
                    if documents:
                        st.success(f"âœ… ã‚µãƒ³ãƒ—ãƒ«å–å¾—æˆåŠŸ: {len(documents)}ä»¶")
                        
                        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åˆ†æ
                        source_page_analysis = {}
                        for meta in metadatas:
                            if meta:
                                source = meta.get('source', 'unknown')
                                page = meta.get('page', 'unknown')
                                kind = meta.get('kind', 'unknown')
                                
                                key = f"{source}_p{page}_{kind}"
                                source_page_analysis[key] = source_page_analysis.get(key, 0) + 1
                        
                        st.markdown("**ã‚µãƒ³ãƒ—ãƒ«åˆ†æ:**")
                        for key, count in source_page_analysis.items():
                            if count > 1:
                                st.warning(f"âš ï¸ {key}: {count}ä»¶")
                            else:
                                st.success(f"âœ… {key}: {count}ä»¶")
                    
                    # IDé‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if ids:
                        unique_ids = len(set(ids))
                        total_ids = len(ids)
                        
                        if unique_ids != total_ids:
                            st.error(f"âŒ IDé‡è¤‡æ¤œå‡º: ç·æ•°{total_ids}, ãƒ¦ãƒ‹ãƒ¼ã‚¯{unique_ids}")
                        else:
                            st.success(f"âœ… IDé‡è¤‡ãªã—: {unique_ids}ä»¶")
                            
            except Exception as e:
                st.error(f"è¨ºæ–­ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                
        else:
            st.warning("RAGã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    # å†åˆæœŸåŒ–ï¼ˆãƒ‡ãƒãƒƒã‚°ç‰ˆï¼‰
    if st.button("ğŸ”„ ãƒ‡ãƒãƒƒã‚°ç‰ˆã§å†åˆæœŸåŒ–"):
        with st.spinner("ãƒ‡ãƒãƒƒã‚°ç‰ˆã§å†åˆæœŸåŒ–ä¸­..."):
            try:
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
                st.session_state.rag_collection = None
                st.session_state.rag_files = []
                st.session_state.last_rag_sources = []
                st.session_state.last_rag_images = []
                
                # ãƒ‡ãƒãƒƒã‚°ç‰ˆã§åˆæœŸåŒ–
                res = initialize_chroma_from_input_debug(
                    input_dir="rag_data",
                    persist_dir=None,
                    collection_name="session_docs"
                )
                
                st.session_state.rag_collection = res["collection"]
                st.session_state.rag_files = res["rag_files"] 
                st.session_state.debug_info = res.get("debug_info", {})
                
                st.success("âœ… ãƒ‡ãƒãƒƒã‚°ç‰ˆã§å†åˆæœŸåŒ–å®Œäº†ï¼")
                st.info("ã€ŒğŸ“‹ åˆæœŸåŒ–ãƒ­ã‚°ã‚’è¡¨ç¤ºã€ã§è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
                
            except Exception as e:
                st.error(f"å†åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                import traceback
                st.text(traceback.format_exc())

# ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã®ä¿®æ­£
# startup_loader.pyã®é–¢æ•°ã‚’ç½®ãæ›ãˆã‚‹å ´åˆï¼š
def initialize_chroma_from_input(input_dir: str, persist_dir: str | None, collection_name: str = "default") -> dict:
    """
    å…ƒã®é–¢æ•°ã‚’ãƒ‡ãƒãƒƒã‚°ç‰ˆã«ç½®ãæ›ãˆ
    """
    return initialize_chroma_from_input_debug(input_dir, persist_dir, collection_name)